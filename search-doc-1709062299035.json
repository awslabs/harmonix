{"searchDocs":[{"title":"Create custom templates","type":0,"sectionRef":"#","url":"/docs/diy/create-custom-template","content":"Create custom templates Work in progress","keywords":"","version":"Next"},{"title":"Contribution","type":0,"sectionRef":"#","url":"/docs/diy/contribution","content":"Contribution Work in progress","keywords":"","version":"Next"},{"title":"Changelog","type":0,"sectionRef":"#","url":"/docs/CHANGELOG","content":"","keywords":"","version":"Next"},{"title":"0.3.3 - 2024-02-26​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#033---2024-02-26","content":" ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#new-features","content":" Amazon ECS using EC2 nodes sample added to reference templateAdd check for cdk bootstrap during environment provider provisioning to avoid manual pre-reqGenAI runtime environment provider templateGenAI RAG application template  ","version":"Next","tagName":"h3"},{"title":"Bug Fixes​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#bug-fixes","content":" Ensure Terraform state bucket name uniquenessHard-coded region in EKS providercode hygiene  ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#documentation","content":" Security and Permissions updatesKubernetes runtime and template docsContribution guidanceArchitecture description and diagram improvementsGeneral fixes to typos and clarificationsGenAI runtime and template docs  ","version":"Next","tagName":"h3"},{"title":"0.3.2 - 2024-01-29​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#032---2024-01-29","content":" ","version":"Next","tagName":"h2"},{"title":"Fixed​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#fixed","content":" fixes reference repository pipelines to correct build issuesfixes installation doc image referencing GitLab 16.8.1 in the AWS Marketplace  ","version":"Next","tagName":"h3"},{"title":"0.3.1 - 2024-01-24​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#031---2024-01-24","content":" ","version":"Next","tagName":"h2"},{"title":"Fixed​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#fixed-1","content":" Backstage plugin version identifiers required for 0.3.0 releaseintroduces a workaround for GitLab 16.8 error when creating new userintroduces a workaround for an image build error due to upgraded &quot;swagger-ui-react&quot; module  ","version":"Next","tagName":"h3"},{"title":"0.3.0 - 2024-01-23​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#030---2024-01-23","content":" ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#new-features-1","content":" Support for Kubernetes: Add Amazon EKS providerImport existing Amazon EKS clustersAdd Amazon EKS Application for K8s Kustomize patternAdd Amazon EKS Application for K8s Helm patternAdd CI/CD patterns for K8s applicationsNew UI Page for K8s with control panel to operate the application S3 Bucket as a shared resourceReuse existing VPC when creating providers(Import existing VPC)Amazon ECS provider with EC2 clusters for tailored workloadsUpdated Backstage platform to v1.21Support filter relevant environments for new apps  ","version":"Next","tagName":"h3"},{"title":"New Documentation​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#new-documentation","content":" Security documentationTest casesAdd a basic provider template - example for starting your own provider templates.  ","version":"Next","tagName":"h3"},{"title":"Refactor​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#refactor","content":" Remove hyphenated names from entitiesAdding support for component subType for easy identification of internal AWS component classification  ","version":"Next","tagName":"h3"},{"title":"Bug Fixes​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#bug-fixes-1","content":" Delete provider - case name issueDelete app - case name issueDelete resource - remove secret deletion + adjusted stack name mappingRemove provider from an environment after creation  ","version":"Next","tagName":"h3"},{"title":"0.2.0 - 2023-09-26​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#020---2023-09-26","content":" ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#new-features-2","content":" Environment and Environment provider entity typesMulti-account and multi-region supportEnvironment and Environment provider provisioning from BackstageGitLab pipelines for CI/CD to build and deploy applicationsDynamic pipelines for new environment deploymentsEnvironment addition at application levelIntroduce environment hierarchy/level and require approval options for AWS EnvironmentApplication auditing per environmentEnvironment selector drop down - Contextual environment switching for applicationsResource template for RDSResource Binding to applicationsAWS-Resource Backstage PageApp Pending page - while pipeline is still provisioning the app/resourceJava SpringBoot templateDelete App &amp; Delete Provider capabilitiesServerless and ECS environment provider examplesPermissions framework adoptionInstallation improvements  ","version":"Next","tagName":"h3"},{"title":"0.1.0 - 2023-04-10​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#010---2023-04-10","content":" ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Changelog","url":"/docs/CHANGELOG#new-features-3","content":" initial release ","version":"Next","tagName":"h3"},{"title":"Create pipelines patterns","type":0,"sectionRef":"#","url":"/docs/diy/create-pipelines-patterns","content":"Create pipelines patterns Work in progress","keywords":"","version":"Next"},{"title":"Customizing security","type":0,"sectionRef":"#","url":"/docs/diy/customizing-security","content":"Customizing security Work in progress","keywords":"","version":"Next"},{"title":"Design your platform","type":0,"sectionRef":"#","url":"/docs/diy/design-your-platform","content":"Design your platform Work in progress","keywords":"","version":"Next"},{"title":"CONTRIBUTING","type":0,"sectionRef":"#","url":"/docs/CONTRIBUTING","content":"","keywords":"","version":"Next"},{"title":"Contributing​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#contributing","content":" Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.  Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.  ","version":"Next","tagName":"h2"},{"title":"Reporting Bugs/Feature Requests​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#reporting-bugsfeature-requests","content":" We welcome you to use the GitHub issue tracker to report bugs or suggest features.  When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:  A reproducible test case or series of stepsThe version of our code being usedAny modifications you've made relevant to the bugAnything unusual about your environment or deployment  ","version":"Next","tagName":"h3"},{"title":"Contributing via Pull Requests​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#contributing-via-pull-requests","content":" Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:  You are working against the latest source on the main branch.You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.You open an issue to discuss any significant work - we would hate for your time to be wasted.  To send us a pull request, please:  Fork the repository.Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.Ensure local tests pass.Commit to your fork using clear commit messages.Send us a pull request, answering any default questions in the pull request interface.Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.  GitHub provides additional document on forking a repository andcreating a pull request.  ","version":"Next","tagName":"h3"},{"title":"Finding contributions to work on​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#finding-contributions-to-work-on","content":" Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.  ","version":"Next","tagName":"h3"},{"title":"Code of Conduct​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#code-of-conduct","content":" This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contactopensource-codeofconduct@amazon.com with any additional questions or comments.  ","version":"Next","tagName":"h3"},{"title":"Security issue notifications​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#security-issue-notifications","content":" If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.  ","version":"Next","tagName":"h3"},{"title":"Licensing​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#licensing","content":" See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.  ","version":"Next","tagName":"h3"},{"title":"Contributing Assets to OPA on AWS​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#contributing-assets-to-opa-on-aws","content":" Thank you for considering a contribution to OPA on AWS project! The above described guidelines are to set the standard of submitting Pull Request, That is in conjunction to the below description  ","version":"Next","tagName":"h2"},{"title":"Contribution Type​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#contribution-type","content":" Contributing OPA on AWS Templates Provider TemplateApp TemplateResource TemplateOther Template Contributing OPA on AWS Pipelines New pipeline patternUpdate existing pipeline pattern Contributing OPA on AWS Core modifications Change in UI / Frontend of OPA on AWS pluginsChange in SDK API / Backend of OPA on AWS PluginsChange in sacffolder actionsChange in architecture or platform designChange in common interfaces Contributing OPA on AWS extensions Integration with new tools and pluginsIntegration with additional Backstage.io APIs / entitiesExtending OPA on AWS model  ","version":"Next","tagName":"h3"},{"title":"Submitting Contribution​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#submitting-contribution","content":" Before submitting any contribution type please make sure it adheres to the OPA on AWS architecture  ","version":"Next","tagName":"h3"},{"title":"Contributing an OPA on AWS provider​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#contributing-an-opa-on-aws-provider","content":" Questions to consider when designing new provider:  Why do i need a new provider? does the existing providers support the type of application I'm trying to build?What is the common environment this provider support for applications?Does the new provider type supports multiple applications that share the same requirements?Do i have clarity on the roles and permissions required to implement this provider?Do i have clarity on the resources and architecture required to implement this provider?Will this provider support backwards upgrades? what will be the effect of applications if the provider were to be updated?  Build your provider​  Step 1 Start with designing the architecture that will meet the particular type of workloads this provider needs to support. Express the architecture with your choice of IAC(CDK, TF, Pulumi etc.)  example: building an ECS provider - will require an AWS ECS cluster - which also requires a VPC and support for logs, encryption and load balancer to allow access to the containers. an AWS ECR is also required to store the container images.  Step 2 Define the Provisioning role and Operations role permissions - this needs to be reason with the expected user interactions and IAC requirements. It is best practice to set your roles with least privileges permissions.  Step 3 Configure an appropriate pipeline to deploy and update this provider  Step 4 Create a provider backstage template and load the template to the backstage-reference repository  tip Don't forget to update all-templates.yaml with your new template path  Test your provider​  Step 1 Make sure you are able to provision your new provider template. We highly recommend to test different context for this step as described in the test-cases  Step 2 Make sure you can update the provider configurations or IAC and the pipeline will apply the changes succufully  Step 3 Add entries in test-cases document for the new provider implemented.  Submit your PR​  Submit a pull request for the new provider following the instructions in this page.  ","version":"Next","tagName":"h3"},{"title":"Contributing an OPA on AWS application​","type":1,"pageTitle":"CONTRIBUTING","url":"/docs/CONTRIBUTING#contributing-an-opa-on-aws-application","content":" Questions to consider when designing a new application pattern:  Who is the team that going to be using this application template? does it address their requirements?What pattern this new application introduce? is there already an existing pattern that can be used?Does this application can use an existing environment or it requires a new environment type / provider? if so, see the above Contributing an OPA on AWS providerWhat kind of permissions and resources this application needs?What kind of operational actions this application need? which of them can be supported through a pipeline and which one needs a UI of platform changes?Will this application support upgrades? what will be the effect of the deployed applications if we were to be update it?  Build your application​  Step 1 Start with designing the architecture that for this type of workload, this include the desired runtime(Java, .Net, python etc.), the resource that compose this application express by IAC(CDK, TF, Pulumi)  example: building an ECS application - will require an AWS ECS Task and Task definition , in addition the application log will need a log group and you may need to add additional supporting resources such as RDS database or S3 bucket. The application IAC will have dependencies on expected resources such as an existing VPC or ECS cluster. this will be provided by the corresponding selected environment and injected to the application repository. the pipeline will stich all this together and express those arguments as environment variables  Step 2 Define the identity of the application in a shape of an IAM role. This application identity role is used not only to describe the current permission the application needs but also the future permission the application may be granted as a result of the resource binding process.  tip Make sure you IAC supports external ingestion of JSON permission policies. See DeclareJSONStatements example here  Step 3 Configure an appropriate pipeline to deploy and update this application  Step 4 Create an application backstage template and load the template to the backstage-reference repository  tip Don't forget to update all-templates.yaml with your new template path  Test your application​  Step 1 Make sure you are able to provision your new application template. We highly recommend to test different context for this step as described in the test-cases. You should also test provisioning another application on the same environment to make sure there's not conflict of configurations and/or resources.  Step 2   Make sure you can update the application configurations or IAC and the pipeline will apply the changes successfullyMake sure you can update the application code /src and CD pipeline will build and deploy the new application  Step 3 Add entries in test-cases document for the new application implemented.  Submit your PR​  Submit a pull request for the new provider following the instructions in this page. ","version":"Next","tagName":"h3"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/faq","content":"FAQ What is OPA on AWS?​ Orchestrate Platforms and Applications (OPA) on AWS is an open source reference implementation that ties together AWS services into an enterprise ready solution. By abstracting AWS services, OPA on AWS allows application developers to focus on what they do best – writing application logic code, all while allowing platform engineering teams to enforce internal guardrails and best practices at scale. OPA on AWS provides a productive and efficient experience for non-cloud developers and allows for integrations with your current tooling. It is fast, safe, fun, and at-scale. What does OPA on AWS do?​ OPA on AWS provides a comprehensive Internal Developer Platform solution. It allows developers to write application logic, on AWS, without having to know how to provision and manage AWS services. OPA on AWS also allows platform engineering teams to easily manage all the resources and applications that run on AWS. What was the motivation behind building OPA on AWS?​ Enterprise are challenged to integrate with AWS while adhering to their standards, security guardrails and compliance. In addition, cloud engineers are a scarce resource that does not scale. Using platform engineering, this gap is shortened, thus enabling enterprise to improve their developer experience and scales the use of AWS Services securely. Is OPA on AWS an AWS Service?​ No, OPA on AWS is not a managed service. It is an open source platform reference implementation supporting integration with AWS Services for enterprises. Since OPA on AWS is using native AWS Services, it provides all the benefits and scale that AWS can offer along with a productive developer experience Who is the audience for OPA on AWS?​ OPA on AWS binds the AWS services into an enterprise ready offering. It was originally built for financial enterprise use cases to accommodate for security and regulatory compliance needs and the necessity for scale and efficiency. It is applicable for every company interested in scaling while still keeping the same tooling and additional internal and external standards. What support do you offer? How can I learn to use OPA on AWS?​ OPA on AWS is accompanied with documentation, detailed ReadMes, and a series of YouTube videos. All of these resources can help you learn more about the solution and get started. For any issue beyond the current available information, you can reach us by opening an issue via GitHub What is OPA on AWS’s license?​ OPA on AWS is licensed as Apache-2.0. What are the benefits for using OPA?​ A better developer experience and faster way to scale the use of AWS Services for enterprises. Please see the full features page What are the requirements to get started on OPA?​ Please read our documentation Getting started What is the price of OPA?​ OPA on AWS is free open source project. However the use of AWS Services is as per the service pricing of each service you use Is there a roadmap?​ Please see the features section under features Why is OPA on AWS integrated with Backstage?​ Backstage.io is a mature CNCF project, and has been widely adopted across the market. By integrating with Backstage.io, users of OPA on AWS can benefit from +100 plugins and a large community (+20k). This allows customers to customize their environments, and keep existing tooling, while still working on AWS. Building on Backstage.io has helped to expedite the journey of OPA on AWS, and bring more value to our customers. How can OPA on AWS accelerate applications migration?​ Please see our Migration page What happens if Backstage.io releases an update? How will OPA on AWS accommodate updates?​ OPA on AWS lists the latest tested version of Backstage.io with OPA on AWS plugins. The solution is decoupled from Backstage.io, therefore it should be possible to upgrade backstage and install the plugins afterwards. However, Backstage.io may introduce breaking changes and therefore we publish the last tested plugin version. How can I report bugs or submit feature requests?​ Please submit any issues and requests via the public repository Will AWS supply templates or examples? How will they be maintained?​ There are many samples and examples provided, please see this table. Will this make the DevOps team obsolete?​ OPA on AWS helps scale DevOps teams and enables the development teams to own their infrastructure and applications and reduce the bottleneck from the DevOps team. How secure is this solution? Has it been reviewed by a third party?​ The solution was designed for a large financial customer adhering to financial services customer regulation and AWS best practices. The solution have been through an AWS security review. How does OPA on AWS differ from AWS console?​ OPA on AWS aims to provide a productive, efficient, and delightful developer experience by providing a simplified way to use AWS. By nature, the AWS console will enable the skilled cloud engineer to fine-tune and define their needs, however, OPA on AWS will enable a persona to build on AWS without having to know how to operate all the knobs and dials. What is an Internal Developing Platform (IDP)?​ Please refer to platformengineering.org Does OPA on AWS have a CLI?​ Not yet - Please see features Have an additional question? Reach out to us at fsi-pace-pe@amazon.com","keywords":"","version":"Next"},{"title":"Features","type":0,"sectionRef":"#","url":"/docs/features","content":"","keywords":"","version":"Next"},{"title":"Templates​","type":1,"pageTitle":"Features","url":"/docs/features#templates","content":" ","version":"Next","tagName":"h2"},{"title":"Providers​","type":1,"pageTitle":"Features","url":"/docs/features#providers","content":"    ","version":"Next","tagName":"h3"},{"title":"AWS Shared Resources​","type":1,"pageTitle":"Features","url":"/docs/features#aws-shared-resources","content":"    ","version":"Next","tagName":"h3"},{"title":"Apps​","type":1,"pageTitle":"Features","url":"/docs/features#apps","content":"    ","version":"Next","tagName":"h3"},{"title":"Platform Engineer Features​","type":1,"pageTitle":"Features","url":"/docs/features#platform-engineer-features","content":" ","version":"Next","tagName":"h2"},{"title":"Add/Remove providers​","type":1,"pageTitle":"Features","url":"/docs/features#addremove-providers","content":" Allow the platform engineer to configure environments with multiple providers.    Use case: a platform engineer design an environment where application should be running on two regions for resiliency purposes. Two providers with different regions can be associated to the same environment.    Use case: a platform engineer design an environment where application needs to be access by local regional teams. Two providers with different regions can be associated to the same environment.  info Once at least one provider is selected, you can add more providers of the same type: ie: ECS-&gt;ECS, EKS-&gt;EKS, Serverless-&gt;Serverless      ","version":"Next","tagName":"h3"},{"title":"Delete providers / environments​","type":1,"pageTitle":"Features","url":"/docs/features#delete-providers--environments","content":" Allow the platform engineer to delete a provider or environment and their associated resources.  \t  ","version":"Next","tagName":"h3"},{"title":"Segregated auditing​","type":1,"pageTitle":"Features","url":"/docs/features#segregated-auditing","content":" Allow separation of audit logs for each environment. Audit logs are created only for the applications which run on the particular environment provider.  ","version":"Next","tagName":"h3"},{"title":"Deployment requires approval​","type":1,"pageTitle":"Features","url":"/docs/features#deployment-requires-approval","content":" Configure additional control for sensitive environment when automated deployment is not desired. If requires approval is set to the environment - all application pipelines deployed on this environment will require approval before proceeding.    ","version":"Next","tagName":"h3"},{"title":"Application developer Features​","type":1,"pageTitle":"Features","url":"/docs/features#application-developer-features","content":" ","version":"Next","tagName":"h2"},{"title":"Resource binding​","type":1,"pageTitle":"Features","url":"/docs/features#resource-binding","content":" Allow the application developer to bind shared AWS resources to an application to it can access the shared resource.    ","version":"Next","tagName":"h3"},{"title":"Deploy application to additional environments​","type":1,"pageTitle":"Features","url":"/docs/features#deploy-application-to-additional-environments","content":" Allow the application developer to deploy an application to additional environments    ","version":"Next","tagName":"h3"},{"title":"Toggle between the environments​","type":1,"pageTitle":"Features","url":"/docs/features#toggle-between-the-environments","content":" Allow the application developer to toggle between the environments of which an application is deployed and present relevant information for the current selected environment    ","version":"Next","tagName":"h3"},{"title":"Delete an App​","type":1,"pageTitle":"Features","url":"/docs/features#delete-an-app","content":" Delete an application from one or many environments.    ","version":"Next","tagName":"h3"},{"title":"App Development​","type":1,"pageTitle":"Features","url":"/docs/features#app-development","content":" Clone repository: The ability to clone the repository immediately after it was created without waiting for the pipeline to provision the infrastructureStart &amp; stop App: For containerize application - the ability to stop / start the application.Update &amp; delete App: For Serverless application - deploy or remove the APIChange application environment variablesView related AWS app cloud resources: View related application resources that have been provisioned for this appView application logs: View the CloudWatch logs for the applicationVisualize application software catalog relationship: View the application relationships including environments, shared resources and ownersView application CI/CD pipelines: View the application CI/CD pipelines ","version":"Next","tagName":"h3"},{"title":"Installation","type":0,"sectionRef":"#","url":"/docs/getting-started/deploy-the-platform","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Installation","url":"/docs/getting-started/deploy-the-platform#prerequisites","content":" ","version":"Next","tagName":"h2"},{"title":"Software prerequisites​","type":1,"pageTitle":"Installation","url":"/docs/getting-started/deploy-the-platform#software-prerequisites","content":" The following software is required to perform the installation of the platform solution:  Unix-based operating system (Linux, MacOS, or the Windows Subsystem for Linux)node.js - 18.19 or higheryarn - v1.x (Yarn classic)aws-cliaws-cdkjqdockergit  note The installation instructions documented here were tested using the following versions: node v18.19aws-cdk v2.95yarn 1.22.21  ","version":"Next","tagName":"h3"},{"title":"Solution Platform prerequisites​","type":1,"pageTitle":"Installation","url":"/docs/getting-started/deploy-the-platform#solution-platform-prerequisites","content":" Prior to installing the OPA solution platform, you will need to ensure that the following items are configured and available:  AWS Account ID and region - The solution will be installed into an AWS account and region. You will need the 12-digit account ID and must be able to log into the account with sufficient permissions to provision infrastructure resources. GitLab Community Edition EC2 AMI id - The solution will install a small GitLab instance where application source code will be stored. The AWS Marketplace provides a free, community edition of GitLab used by the solution. You will need to subscribe to the AWS Marketplace offering. Search for &quot;GitLab Community Edition&quot; by GitLab or use a direct link: https://aws.amazon.com/marketplace/pp/prodview-w6ykryurkesjqOnce your account is subscribed to the GitLab CE Marketplace offering, save the EC2 AMI for the appropriate region from the &quot;Launch new instance&quot; page as shown in the image below (do not actually launch an instance as this will be done for you during installation). Alternatively, you can query for the AMI using the AWS CLI (substitute the appropriate region value for the --region option): aws ec2 describe-images --owners &quot;aws-marketplace&quot; --filters &quot;Name=name,Values=*GitLab CE 16.8.1*&quot; --query 'Images[].[ImageId]' --region &lt;AWS_REGION&gt; --output text GitLab Runner image - The solution will set up an EC2 instance as a GitLab Runner to execute GitLab CI/CD pipelines. The Amazon-provided &quot;Jammy&quot; image will be used for the runner image. Save the EC2 AMI for the appropriate region for this AMI. The following AMI command will return the appropriate image id. Replace the value for &quot;--region&quot; to reflect your target region: aws ec2 describe-images --owners &quot;amazon&quot; --filters &quot;Name=name,Values=*ubuntu-jammy-22.04-amd64-server-20230208*&quot; --query 'Images[].[ImageId]' --region &lt;AWS_REGION&gt; --output text Route 53 Hosted Zone - The solution will ensure secure communcations and set up a certificate for your defined domain. Ensure that a public hosted zone is set up in your account. See the AWS documentation for creating a public hosted zone Okta authentication - The solution uses Okta and RoadieHQ Backstage plugins for authentication of users and groups. You will need a client id, client secret, and API key for configuration of the solution. If you wish to use Okta for authentication and do not have an existing account, you can sign up a free Workforce Identity Cloud developer account. Once the account is set up, you will need to configure an Okta API key for the RoadieHQ backend catalog pluginA client id, secret and audience are required to set up a Backstage Okta authentication provider. See the Backstage Okta auth documentation for more details.Other identity providers are supported and could be substituted using different plugins. Configuring alternative authentication is not covered in this documentation. Refer to the Backstage Authentication documentation for details to install and configure alternative providers.  ","version":"Next","tagName":"h3"},{"title":"Installation​","type":1,"pageTitle":"Installation","url":"/docs/getting-started/deploy-the-platform#installation-1","content":" Clone the repository and change to the repository location git clone https://github.com/awslabs/app-development-for-backstage-io-on-aws.git cd app-development-for-backstage-io-on-aws Configure the solution Copy the config/sample.env file to config/.envEdit the config/.env file and provide values for all of the environment variables. The file is commented to explain the purpose of the variables and requires some of the information from the Solution Platform Prerequisites section above. info The SECRET_GITLAB_CONFIG_PROP_apiToken, OKTA_IDP and OKTA_AUTH_SERVER_ID variables do not need to be provided. This will be automatically configured during installation after the platform is deployed. Perform the installation Run make install The Makefile target will automatically perform the following actions: Install and configure Backstage Install/update CDK Deploy the solution platform AWS infrastructure Update the configuration with GitLab information Push a sample repository to GitLab Build and deploy the Backstage image to AWS After the installation completes, the application will start up. Open a browser and navigate to the 'OPA on AWS' endpoint using the Route 53 hosted zone name that you configured (e.g. https://${R53_HOSTED_ZONE_NAME}). If any errors occur during installation, please review the install_{datestamp}.log file for details. a new secret manager's secret named opa-admin-gitlab-secrets contains the Gitlab admin's credentials for  ","version":"Next","tagName":"h2"},{"title":"Installation FAQs​","type":1,"pageTitle":"Installation","url":"/docs/getting-started/deploy-the-platform#installation-faqs","content":" I don't use Okta. Can i change the identity provider to another one?  Yes. Backstage supports many identity providers. Once you configure Backstage for your chosen provider, make sure the Backstage catalog is synced with the users and groups from your provider.    I want to use another source control that is not GitLab. How can i do that?  Backstage supports multiple source control providers which can be integrated through the Backstage config. OPA uses GitLab for several usage scenarios which you will need to migrate to another source control provider: Storing application source codeStoring template source codeStoring pipelines jobs and orchestrationUpdate the Client API plugin that interacts with GitLab to the new source control provider    I'm using Terraform, can I use this solution with Terraform to provision application resources?  Yes. We provide a Node.js Terraform application software template for demonstration. You may also write your own provider with Terraform.    For more Q &amp; A please see our FAQ Page ","version":"Next","tagName":"h2"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/intro","content":"","keywords":"","version":"Next"},{"title":"What is OPA on AWS?​","type":1,"pageTitle":"Intro","url":"/docs/intro#what-is-opa-on-aws","content":" Orchestrate Platforms and Applications (OPA) on AWS brings the AWS cloud closer to your developers. The platform allows enterprise customers to build environments and applications on AWS without requiring application developers to upskill on cloud expertise. OPA on AWS is a reference implementation for an enterprise-grade, fully integrated internal developer platform. It improves the workflow for application developers, with a secure and scalable experience for non-cloud developers.  OPA on AWS is built on a Cloud Native Computing Foundation (CNCF) project, Backstage, which is an open platform for building developer portals. The Backstage platform itself has been adopted by over 900 companies as their primary developer portal in the past 3 years and has over 100 plugins available for its end users. OPA on AWS takes Backstage to the next level by seamlessly integrating it with AWS and packaging it together for enterprise use.  ","version":"Next","tagName":"h2"},{"title":"Why did we build OPA on AWS?​","type":1,"pageTitle":"Intro","url":"/docs/intro#why-did-we-build-opa-on-aws","content":" Customers usually start their journey on AWS with small groups, which usually consist of highly skilled individuals who are well-versed in cloud technologies. However, scaling that experience is the true challenge. Providing a better developer experience can help customers in their cloud adoption journey.  This challenge is even more difficult for enterprise customers that need to meet their organization's operational, security, and compliance standards. Platform engineering helps to reduce this bottleneck and to enable enterprises to scale and improve their developer experience and the use of AWS Services securely.  The below image depicts how organizations have evolved from slow and manual processes into modern platforms that streamline application development.    ","version":"Next","tagName":"h2"},{"title":"How does it work?​","type":1,"pageTitle":"Intro","url":"/docs/intro#how-does-it-work","content":" OPA on AWS provides the provisioning and operational layers to build applications quickly and security via a self-service internal developer portal. It leverages existing AWS Services like EKS and ECS so that users can enjoy the agility and scalability of these services. It’s the best of both worlds - a customizable developer platform with the power and scale of AWS Services. To get you started quickly, we provide dozens of templates and patterns we have collected from different teams and customers.    ","version":"Next","tagName":"h2"},{"title":"What's included?​","type":1,"pageTitle":"Intro","url":"/docs/intro#whats-included","content":" The open source solution includes the following:  Complete source code to customize your own platform.Dozens of templates and examples to create environments, providers, applications and AWS resources.Backstage plugins and plugins source code.Predefined CI/CD centralized pipelines for more than a dozen common patterns.Documentation, support videos, and a workshop.  Refer to the features page for a complete list of capabilities.  ","version":"Next","tagName":"h2"},{"title":"Getting Started​","type":1,"pageTitle":"Intro","url":"/docs/intro#getting-started","content":" Get started by creating a new platform ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/migrations/overview","content":"","keywords":"","version":"Next"},{"title":"Understanding Application Components​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#understanding-application-components","content":" To successfully migrate an application we first have to understand all the application components, dependencies, and composition.  Here are some examples of common application components:  Application source code.Application-related resources, such as: databases, caches, queues, streams etc.Shared resources, such as databases that are referenced by multiple applicationsApplication configuration: port, database url, keys, environment variables etc.Infrastructure configuration to support the application: required CPU, RAM, runtime environment, networking, exposure, resiliency configurations etc.CICD: pipelines and automation for continuous integration and continuous deploymentApplication dataFirewall configurationComposition and relations to other applications and resourcesSecurity related components - encryption, required security scansObservability and logging  ","version":"Next","tagName":"h2"},{"title":"Setting up Target Components​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#setting-up-target-components","content":" Before we can look at mapping the application source components to a migration plan, we need to prepare the target environment's application specifications.  Prior to the steps below, a landing zone and organizational foundation needs to be created. Many customers use AWS Control Tower to provide an easy landing zone with guardrails and controls built in. It may also be considered to introduce centralized networking components, observability and monitoring across the organization and integration with on-premises resources. This article does not address this process.  ","version":"Next","tagName":"h2"},{"title":"Step 1 - Target Cloud Infrastructure​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#step-1---target-cloud-infrastructure","content":" Based on the type of application, we need to prepare target environments on AWS. Items to decide upon include:  The account(s) that will be usedThe region(s) for the applicationHow many environments are needed? Dev, Test, Staging, ProdUnderlying network topology - VPCThe type of application - Containerized, Serverless, Legacy EC2, OtherThe technology to implement the infrastructure runtime: EKS, ECS, Serverless, Classic EC2 etc.  In addition, we will need to estimate the number of applications that may share the same environment - this will influence the capacity requirement and the default configurations per app. If we use serverless solutions, this step may be easier as capacity is on-demand.  All of these requirements will be encapsulated in Infrastructure as Code (IaC) that can be provisioned based on the desired arguments (see above).  The result of this step is one or more templates of AWS Environment and AWS Environment Providers that will support our future migrated applications.  ","version":"Next","tagName":"h3"},{"title":"Step 2 - Target Application Template​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#step-2---target-application-template","content":" We will need to define the application resources (IaC) that compose our application, including the task / process that will run it all the way to the required application runtime (e.g. Java, Python, Node, .Net).  In addition, we will need to know the estimated configuration for the application in terms of: CPU, Memory and storage. Lastly, we will need to set the application configuration such as: environment variables, application logic configuration and metadata.  The result of this step is a template for this type of application pattern that relies on the infrastructure of step 1 but also captures all the required resources for the application:  Application identity / roleTask definition / container specificationSecret and configurationsOutput log configurationSupporting infrastructure - buckets, log groups, tags  ","version":"Next","tagName":"h3"},{"title":"Step 3 - Additional Shared Resources​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#step-3---additional-shared-resources","content":" Now we need to find out if the application requires other resources such as: databases, S3 buckets, shared file systems, queues, streams etc.  If it does, we will need to create a shared resource template for each one, including IaC and support for future policy permissions grants. See this example.  ","version":"Next","tagName":"h3"},{"title":"Step 4 - Pipelines​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#step-4---pipelines","content":" In order to orchestrate the templates above, we will need to build appropriate pipelines for each one of them.  The expression of the pipeline is based on your CI/CD tooling and should be generalized to the the technology that is deployed or application use case. The implementation should include  IAC deployment pipelineApplication deployment pipeline  ","version":"Next","tagName":"h3"},{"title":"Orchestrating Migration Automation​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#orchestrating-migration-automation","content":" Let's look at the components we have built so far. Every template has IaC that gets provisioned through the pipeline and deploys the resource on our target account:    info Orchestrating the creation of these components needs to be carefully considered as order matters and we can only apply certain configurations after a dependent template is properly provisioned.  ","version":"Next","tagName":"h2"},{"title":"Automating and Creating target components​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#automating-and-creating-target-components","content":" Because we can only provision an application to an existing environment - we must first create the environment and only then deploy the application. However, while creating the application we may need to attach shared resources; therefore, we may want to create shared resources before the application is created. Lastly, once the application is created, we can apply the configuration required before starting the migration process.    ","version":"Next","tagName":"h3"},{"title":"Migration Process​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#migration-process","content":" If we put all these steps together to create an ordered migration plan, we will assemble a two-phase migration process.  App deployment: prepare all target resources (environment, provider, application and shared resources)App migration: migrate the source code and data of the application    ","version":"Next","tagName":"h3"},{"title":"Migration Validation​","type":1,"pageTitle":"Overview","url":"/docs/migrations/overview#migration-validation","content":" Lastly, we would want to validate the migrated applications. There are several approaches to automate the validation process.  Running unit tests against the new target application endpointData validation and data integrity testingIntegration testing - Application access and cross application communicationScaling and performance testingResiliency testing  tip Leveraging the platform is a good practice as it not only provides information about the deployed application, but it also provides information about the relationship, the environment, and a single place to query all of this data through standardized APIs. ","version":"Next","tagName":"h3"},{"title":"Architecture","type":0,"sectionRef":"#","url":"/docs/techdocs/architecture","content":"","keywords":"","version":"Next"},{"title":"OPA Platform​","type":1,"pageTitle":"Architecture","url":"/docs/techdocs/architecture#opa-platform","content":" The below diagram illustrates the major components of the OPA platform.  The platform creation is automated by way of executing an AWS CDK script that will provision the needed resources in your AWS account. After running the script, you will have the Backstage developer portal running on AWS and it will be set up to persist its configurations to an RDS database. Backstage will be integrated with an identity provider to facilitate user logins. The default identity provider is Okta, but you can customize this to use a different one.  Backstage is also integrated with a version control system. It is configured to discover entity definition files in existing Git repositories so that these entities will show up in the portal. Backstage will also be able to create new repositories to hold the source code of applications and other resources that are created by portal users.  The default OPA version control system is GitLab. The platform creation scripts will set up a Community Edition of GitLab that runs on AWS, so that it can be used for demonstration purposes. It is possible through code modifications to switch to a different version control vendor that can be hosted on or outside of the AWS cloud.      ","version":"Next","tagName":"h2"},{"title":"Application Environments​","type":1,"pageTitle":"Architecture","url":"/docs/techdocs/architecture#application-environments","content":" The below diagram illustrates the relationships between an application and the environments it gets deployed to.  An environment is just a Backstage entity that contains metadata and associations with other entities such as applications and providers. Environment providers are more than just Backstage entities. Providers are backed by Git repositories that contain Infrastructure as Code scripts that provision resources to a specific AWS account and region. The provider's resources are created to support running applications of a certain type, such as containerized apps that will be run on a cluster or serverless apps.      ","version":"Next","tagName":"h2"},{"title":"AWS Cloud Components of an Environment Provider​","type":1,"pageTitle":"Architecture","url":"/docs/techdocs/architecture#aws-cloud-components-of-an-environment-provider","content":" The below diagram illustrates the AWS cloud components that make up an environment provider that could be used to host a containerized application.  Every provider must include a security role for Backstage CICD pipelines to use to provision and update applications in a specific AWS account and region. Providers's must also contain an operations role, which is assumed by the Backstage portal to grant it access to execute the operations that portal users are choosing. The final requirement of a provider is that it must contain an audit table, which OPA uses to track user actions made to applications running on the provider's account.  Providers can also commonly include networking configurations, encryption keys, and secrets.      ","version":"Next","tagName":"h2"},{"title":"How an Application Gets Created​","type":1,"pageTitle":"Architecture","url":"/docs/techdocs/architecture#how-an-application-gets-created","content":" Applications are created when developers log into Backstage and select an existing application template that meets their needs. They are then asked to fill in a form with vital information, such as the application's name and the environment it will run on.  Backstage will create a new Git repository to hold the application's code, its CICD pipeline definition, and its Backstage entity definition. Backstage reads the entity definition so that the new application will show up on the portal.  When the Git repository is created, its CICD pipeline will execute, resulting in resources being provisioned on AWS. Once the pipeline has finished, Backstage users will be able to see many attributes of the application, such as its running state and logs. They will also be able to perform operations on the application, such as changing environment variables or pushing a new release.      ","version":"Next","tagName":"h2"},{"title":"Application CICD Pipeline​","type":1,"pageTitle":"Architecture","url":"/docs/techdocs/architecture#application-cicd-pipeline","content":" The below diagram illustrates an application's CICD pipeline. The application is associated with 3 environments (DEV, QA, PROD).  OPA is able to create pipelines like this for you when it creates a new application. These pipelines are capable of deploying to as many AWS accounts as you need.     ","version":"Next","tagName":"h2"},{"title":"Plugins","type":0,"sectionRef":"#","url":"/docs/techdocs/plugins","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Plugins","url":"/docs/techdocs/plugins#architecture","content":"     Backstage Plugins description  Plugin\tType\tDescriptionBackend - @aws/plugin-aws-apps-backend-for-backstage\tBackstage backend plugin\tresponsible for all outgoing API calls to AWS, SCM (Gitlab) and the platform Frontend - @aws/plugin-aws-apps-for-backstage\tBackstage frontend plugin\tresponsible for all UI interaction components - pages, components, state and frontend API Common - @aws/plugin-aws-apps-common-for-backstage\tBackstage isomorphic plugin\tshareable plugin for both frontend, backend and scaffolder, used to define interfaces and types Scaffolder - @aws/plugin-scaffolder-backend-aws-apps-for-backstage\tBackstage scaffolder plugin\tcontains the actions required to execute the templates  tip For more information on backstage plugins visit Backstage plugin documentation  ","version":"Next","tagName":"h2"},{"title":"Plugin dependencies​","type":1,"pageTitle":"Plugins","url":"/docs/techdocs/plugins#plugin-dependencies","content":"   ","version":"Next","tagName":"h2"},{"title":"Plugin source code​","type":1,"pageTitle":"Plugins","url":"/docs/techdocs/plugins#plugin-source-code","content":" Backend pluginFrontend pluginCommon pluginScaffolder plugin ","version":"Next","tagName":"h2"},{"title":"Deploying Your Application","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/generative-ai/deployingYourApp","content":"","keywords":"","version":"Next"},{"title":"Creating a Gen AI Environment Runtime​","type":1,"pageTitle":"Deploying Your Application","url":"/docs/techdocs/runtimes/generative-ai/deployingYourApp#creating-a-gen-ai-environment-runtime","content":" Before you can deploy an application to one of your clusters via OPA, you'll need to perform these steps in the OPA/Backstage UI, which are typically performed by someone in the Platform Engineering role:  Create a new Gen AI Environment Provider by selecting the Generative AI provider template. Fill in the required information and wait for your resources to deploy. This may take several minutes. Create a new Environment and select the provider that you just created. This establishes an association between an Environment and a Provider. Optionally, you can view the Environment and the Environment Provider from the OPA/Backstage UI. Doing so will show you valuable information about the entity, such as metadata and relationshipts with other entities. The UI will also provide you with a link to the Git repository that contains the configurations and code (if there is any) for the entity you are viewing. info Environment Providers come with their own CICD pipelines that execute their IaC scripts. These pipelines will run automatically to deploy changes if the IaC scripts change.  ","version":"Next","tagName":"h2"},{"title":"Deploying the Gen AI Application​","type":1,"pageTitle":"Deploying Your Application","url":"/docs/techdocs/runtimes/generative-ai/deployingYourApp#deploying-the-gen-ai-application","content":" Once the Gen AI runtime environment is configured, you are ready to deploy your Gen AI Chatbot App.  Select the Generative AI Chatbot Application software template and enter the information as prompted. Be sure to select the environment that you just created as the target environment for your application. Wait for the pipeline to finish. Progress of the pipeline can be found in GitLab CI/CD. Once the pipeline is completed, navigate to the application's component page on the OPA on AWS UI. This is where you will find all of the important information about your application. Find the entity card that says Application State and select Deploy App. This will trigger the final IaC deployment of your application's resources in AWS CloudFormation. Once the Stack is successfully deployed, you are ready to begin interacting with your application. ","version":"Next","tagName":"h2"},{"title":"Processes","type":0,"sectionRef":"#","url":"/docs/techdocs/process","content":"","keywords":"","version":"Next"},{"title":"Deploying a template​","type":1,"pageTitle":"Processes","url":"/docs/techdocs/process#deploying-a-template","content":" info The process described below is true for all templates with the exception of AWS Environment.  When a user selects a template in the catalog and clicks Create, the below process is initiated.    Template Creation: The process of filling the template with the required input from the user, text fields, selecting other entities, choosing options etc.Rendered Entity Data: Based on the previous selection, this process will fetch all the required data before populating the repository. This includes information about the selected environment entity, platform configurations and entity rendered data. You can see the result of this process in the generated repository right after creation. This process make use of the Scaffolder actions plugin.Create Repository: Create the git repository for the entity, code, IaC and environment information. Based on the type of template, the repository will be created in a git project group: environments, environment-providers, aws-app, aws-resources. This allows us to segregate access control from managing repositories under different security domains.Store initial files and pipeline: Persisting the generated files from step 2 in the repository including the particular pipeline pattern that fits the selected template (See below for more details on pipeline process).Pipeline execution: Using Gitlab runner executing the pipeline in a separate shell. For building container images, we use Docker in Docker with Paketo build packs or KanikoGet Credentials: Fetch credentials from the target (Environment provider) provisioning role in order to provision the IAC for the template.Provision IaC: Provision the IaC against the target destination (Environment provider) - cdk deploy / terraform applyUpdate Entity catalog: Based on step 7 select the desired IaC deployment output and update the entity catalog file (catalog-info.yaml) with the resources information.  ","version":"Next","tagName":"h2"},{"title":"Executing Gitlab pipeline​","type":1,"pageTitle":"Processes","url":"/docs/techdocs/process#executing-gitlab-pipeline","content":" The pipeline execution process is composed of several patterns and abstractions. This was done for several reasons, but mostly for reusability and security controls.  ","version":"Next","tagName":"h2"},{"title":"Abstracted centralized pipelines​","type":1,"pageTitle":"Processes","url":"/docs/techdocs/process#abstracted-centralized-pipelines","content":" In order to allow updates and changes for pipelines that may be enforced by the platform engineering team, we externalized some of the pipeline actions and provide only the necessary variables as an input.  .gitlab-ci.yml stages: - env-creation - prepare-dev-stage - dev-stage variables: APP_SHORT_NAME: &quot;my-app&quot; APP_TEMPLATE_NAME: &quot;example-nodejs&quot; OPA_PLATFORM_REGION: &quot;us-east-1&quot; include: - project: 'opa-admin/backstage-reference' ref: main file: - 'common/cicd/.gitlab-ci-job-defaults-cdk.yml' - 'common/cicd/.gitlab-ci-aws-base.yml' - 'common/cicd/.gitlab-ci-aws-iac-ecs.yml' - 'common/cicd/.gitlab-ci-aws-image-kaniko.yml'   The first part of our pipeline, &quot;stages&quot;, defines the stages of this pipeline for the current state. This part is not fixed, we may introduce more stages when our application will be deployed to additional environments. Nonetheless we can see the first two stages for our dev environment:  prepare-dev-stage - this stage provisions and prepares the environment to be able to run the application before we start making continuous code changes to our repositories.dev-stage - this stage is responsible for compiling, building and deploying new versions of our application each time we push new code changes to our application logic (/src directory)  The env-creation is the stage to create the two stages above, the reason this is a stage on it's own is because we can reuse it to create more stages later, when the application is deployed to other environments.  tip The include clause allows us to dynamically pull pipelines from another repository each time the pipeline will run.  ","version":"Next","tagName":"h3"},{"title":"Modular pipelines​","type":1,"pageTitle":"Processes","url":"/docs/techdocs/process#modular-pipelines","content":" The idea of modular pipelines is to include smaller pipelines that contain jobs which together can provide a building block for different pipeline orchestrations. You may have already noticed from the example above we use several include statements. These statements help to reuse similar functionally for different templates / application patterns.  The complete list of pipeline patterns is available at: CICD Directory  .gitlab-ci-aws-base.yml.gitlab-ci-aws-dind-spring-boot.yml.gitlab-ci-aws-iac-ecs.yml.gitlab-ci-aws-iac-rds.yml.gitlab-ci-aws-iac-serverless-api.yml.gitlab-ci-aws-iac-tf-ecs.yml.gitlab-ci-aws-image-kaniko.yml.gitlab-ci-aws-provider-ecs.yml.gitlab-ci-aws-provider-serverless.yml.gitlab-ci-aws-tf-base.yml.gitlab-ci-job-defaults-cdk.yml.gitlab-ci-job-defaults-tf.yml  ","version":"Next","tagName":"h3"},{"title":"Pipeline jobs​","type":1,"pageTitle":"Processes","url":"/docs/techdocs/process#pipeline-jobs","content":" There are several jobs imported into the pipeline based on the pattern you use. Below is an example of pipeline executions and the job for each one of them:      Initial commit create-ci-stages - The job creates new stages for a target environment with its providers. Added CICD environment stage iac-deployment-ENVXXX-ProviderYYY - the job provision IAC against the target environment/providerget-aws-creds-ENVXXX-ProviderYYY - the job gets credentials from the provisioning role of the target providerbuild-image-ENVXXX-ProviderYYY - the job build a new image from the /src directory and update the container imagedelete-aws-creds-ENVXXX-ProviderYYY - delete the temporary credentials so that they are not persisted in the repository updating entity details - update cataloginfo.yaml with the IaC metadata. There is no need to run the pipeline again after this update, which is why the pipeline is showing as &quot;Skipped&quot;.  ","version":"Next","tagName":"h3"},{"title":"Deploy an application to another environment​","type":1,"pageTitle":"Processes","url":"/docs/techdocs/process#deploy-an-application-to-another-environment","content":" The process of deploying an application to another environment works by utilizing the jobs and stage we describe above. To visualize how the git pipeline looks when performing a multi-environment deployment, let's look at the below diagram.      We can see that once we deploy an application to another environment we essentially created new stages for the new target environment.  tip The process to deploy an application to another environment is done by submitting a commit to git with a specific message generate CICD stages along with a properties file that contains the information of the new target environment. When the pipeline job runs, it will process the new file and create corresponding stages for the new target destination. This change will automatically execute the pipeline and stage to deploy the application to the new target environment.  ","version":"Next","tagName":"h2"},{"title":"Continuous deployment for app code​","type":1,"pageTitle":"Processes","url":"/docs/techdocs/process#continuous-deployment-for-app-code","content":" How do the application logic code changes propagate to multiple environments? For example - if we have the below pattern of commit on our repository - every time we merge changes to the main branch /src directory (Can be configured differently) the ENV-XXX-stage will be triggered.    If we have several stages for multiple environments, all of them will be triggered in the original order they were created in.    note If we configured the environment with &quot;Requires approval&quot;, the pipeline that pushes code changes will halt until an approval is granted. This is useful in cases where we want supervision of what changes are propagated to a sensitive environment. Additional security controls can also be implemented. ","version":"Next","tagName":"h3"},{"title":"Getting Started With An Example","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/generative-ai/gettingStartedWithExample","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Getting Started With An Example","url":"/docs/techdocs/runtimes/generative-ai/gettingStartedWithExample#prerequisites","content":" Please follow the instructions highlighted in the Deploying Your Application page. Once you have successfully deployed your application resources, you may proceed with the following steps.  ","version":"Next","tagName":"h3"},{"title":"Step 1: Set Configuration Parameters​","type":1,"pageTitle":"Getting Started With An Example","url":"/docs/techdocs/runtimes/generative-ai/gettingStartedWithExample#step-1-set-configuration-parameters","content":" Our first step after deploying the resources for our application is to customize the configurations. As part of the deployment process, default values are set for all of the customizable parameters. The default values are as follows:  response_prompt: &quot;Use the following documents to answer the following {question}. Here are the documents, between &lt;document&gt;&lt;/document&gt; XML tags: {documents} Do not directly mention the content of the documents if not relevant to the question. Ensure that your answer is accurate and uses the information from the documents previously provided.&quot; temperature : 1 topics : [ ] (this is currently an empty array that we will modify in Step 2) max_tokens_to_sample : 400 chunk_size : 28000 chunk_overlap : 0 relevant_document_count : 3  For more details on parameter descriptions and requirements, and how to make an API request please see the Step 1: Set Configuration Parameters section on the Interacting With Your Application page.  For our use case, we will start by modifying the topics parameter. Make the following request to the /setConfiguration method of your API:  { &quot;parameters&quot;: { &quot;topics&quot;: &quot;['amazon', 'google', 'meta', 'apple']&quot;, } }   These represent the high level topics that we will be using to organize, classify and store our indexed data by.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Upload Contextual Documents​","type":1,"pageTitle":"Getting Started With An Example","url":"/docs/techdocs/runtimes/generative-ai/gettingStartedWithExample#step-2-upload-contextual-documents","content":" These topics now need to be reflected in our data source's file structure. Locate the sample documents in the source code of your application repo: ./sample-files  Navigate to your S3 Data Bucket and upload these folders and files. The S3 data structure should be as follows:   Your Data Bucket │ ├───amazon/ │ └───Amazon Annual Report 2022.txt │ ├───google/ │ └───Google Annual Report 2022.txt | ├───meta/ │ └───Meta Annual Report 2022.txt | └───apple/ └───Apple Annual Report 2022.txt   Adding these documents is triggering the Indexer Lambda Function, which will be creating and storing the indexes in Amazon OpenSearch Service according to the parameters we set in Step 1. This process will take a couple of minutes. To validate completion, see the Indexer Lambda Function's CloudWatch Logs or navigate to your OpenSearch Collection and verify that 4 indexes have been created.  NOTE: Adding more files under the same folders won't modify the index name, but it will enrich the data.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Ask your Chatbot a Question​","type":1,"pageTitle":"Getting Started With An Example","url":"/docs/techdocs/runtimes/generative-ai/gettingStartedWithExample#step-3-ask-your-chatbot-a-question","content":" At this point, your chatbot has a couple of SEC documents providing additional context to the LLM. Let's ask a question!  Make a request to the /classification method of our API, for example:  { &quot;message&quot;: &quot;What was the 2022 revenue of Amazon?&quot;, &quot;operation_mode&quot;: &quot;inclusive&quot; }   { &quot;message&quot;: &quot;Who is Google's CEO?&quot;, &quot;operation_mode&quot;: &quot;inclusive&quot; }   { &quot;message&quot;: &quot;What is Meta?&quot;, &quot;operation_mode&quot;: &quot;inclusive&quot; }   Your response should be a contextual answer to your question!  ","version":"Next","tagName":"h3"},{"title":"Step 4: Experiment!​","type":1,"pageTitle":"Getting Started With An Example","url":"/docs/techdocs/runtimes/generative-ai/gettingStartedWithExample#step-4-experiment","content":" Now that we have confirmed that your Chatbot is up and running, we encourage you to experiment. Ask different questions, modify your parameters, add more files, etc.  NOTE: If you want to modify chunk_size or chunk_overlap at this point, you will have to create new indices with a different name or delete the previous ones; followed by modifying your topics parameter. For more information see Step 1: Set Configuration Parameters on the Interacting With Your Application page. ","version":"Next","tagName":"h3"},{"title":"Authorization and Permissions","type":0,"sectionRef":"#","url":"/docs/techdocs/permission","content":"","keywords":"","version":"Next"},{"title":"Specifying a policy​","type":1,"pageTitle":"Authorization and Permissions","url":"/docs/techdocs/permission#specifying-a-policy","content":" The OPA on AWS plugins for Backstage provide example permission policies to demonstrate how the permission framework can be leveraged to enforce access controls to specific template types and OPA APIs. In the reference implementation, the policy used by the permission framework is found in the backstage/packages/backend/src/plugins/permission.ts file. By default, an &quot;allow all&quot; sample policy is used. In the example below, the policy configuration is modified to specify an alternative sample policy named OpaSamplePermissionPolicy.  backstage/packages/backend/src/plugins/permission.ts import { createRouter } from '@backstage/plugin-permission-backend'; import { Router } from 'express'; import { PluginEnvironment } from '../types'; - import { OpaSampleAllowAllPolicy } from './OpaSamplePermissionPolicy'; + import { OpaSamplePermissionPolicy } from './OpaSamplePermissionPolicy'; export default async function createPlugin(env: PluginEnvironment): Promise&lt;Router&gt; { return await createRouter({ config: env.config, logger: env.logger, discovery: env.discovery, - policy: new OpaSampleAllowAllPolicy(), + policy: new OpaSamplePermissionPolicy(), identity: env.identity, }); }   ","version":"Next","tagName":"h2"},{"title":"Defining Groups​","type":1,"pageTitle":"Authorization and Permissions","url":"/docs/techdocs/permission#defining-groups","content":" The sample OpaSamplePermissionPolicy policy example assumes that groups named 'admins', 'dev-ops', and 'developers' are created by the provider which populates the catalog with organizational data. The defined organizational groups are used in policy decisions defined in the permission policy function. The sample code for groups can be modified as needed to customize the group names.  backstage/packages/backend/src/plugins/OpaSamplePermissionPolicy.ts // The Group entity ref constants below are based on group identifiers created from the auth IdP or manually created // Update the entity ref identifiers as appropriate to match your Backstage installation const ADMINS_GROUP = stringifyEntityRef({ kind: 'Group', namespace: DEFAULT_NAMESPACE, name: &quot;admins&quot; }); const DEVOPS_GROUP = stringifyEntityRef({ kind: 'Group', namespace: DEFAULT_NAMESPACE, name: &quot;dev-ops&quot; }); const DEVELOPERS_GROUP = stringifyEntityRef({ kind: 'Group', namespace: DEFAULT_NAMESPACE, name: &quot;developers&quot; });   With the groups defined, we can now review and modify the policy decision code to return authorization results to Backstage plugins.  ","version":"Next","tagName":"h2"},{"title":"Restricting access to OPA audit logs​","type":1,"pageTitle":"Authorization and Permissions","url":"/docs/techdocs/permission#restricting-access-to-opa-audit-logs","content":" OPA on AWS provides an example permission to control access to OPA application audit logs using the readOpaAppAuditPermission permission definition (defined in the @aws/plugin-aws-apps-common-for-backstage plugin). The code below is part of the OpaSamplePermissionPolicy.ts permission policy.  Additional permissions for OPA on AWS APIs may be provided in the future. If there is a specific permission required, open an issue or submit a pull request for support.  backstage/packages/backend/src/plugins/OpaSamplePermissionPolicy.ts ... // store the array of entityRefs which allow this user to claim ownership of an entity const ownershipGroups = user?.identity.ownershipEntityRefs || []; // Example permission decision: // ALLOW admin and devops group members to perform any action const allowAllGroups = [ADMINS_GROUP, DEVOPS_GROUP]; if (ownershipGroups.some(x =&gt; allowAllGroups.includes(x))) { return { result: AuthorizeResult.ALLOW }; } // Example permission decision: // DENY audit read access unless the user is a member of Admin or DevOps // The implementation below assumes that prior checks have returned an // 'allow' policy decision for groups other than 'developer' if (isPermission(request.permission, readOpaAppAuditPermission) &amp;&amp; ownershipGroups.includes(DEVELOPERS_GROUP)) { return { result: AuthorizeResult.DENY }; } ...   ","version":"Next","tagName":"h2"},{"title":"Restricting access to templates​","type":1,"pageTitle":"Authorization and Permissions","url":"/docs/techdocs/permission#restricting-access-to-templates","content":" A common use case may be to restrict access to AWS Environment and Environment Provider template to only members of a specific group. For example, you may want Platform Engineers to use these templates, but prevent Application Developers and other groups from creating new AWS infrastructure.  The code snippet below is part of the OpaSamplePermissionPolicy.ts permission policy and shows how a conditional decision can be returned for templates of type aws-environment or aws-environment-provider. Unless the user is an owner of the template, they will not be allowed to view or execute these software template types (&quot;ownership&quot; is either through direct user ownership or a member of the group that owns the template).  backstage/packages/backend/src/plugins/OpaSamplePermissionPolicy.ts ... // Example permission decision: // Multiple groups of permission decisions can be nested under the first check to ensure we're working with catalog entities if (isResourcePermission(request.permission, RESOURCE_TYPE_CATALOG_ENTITY)) { // Example permission decision: // DENY users access to software templates of type 'aws-environment' or // 'aws-environment-provider' if they cannot claim ownership of the entity if (isPermission(request.permission, catalogEntityReadPermission)) { return createCatalogConditionalDecision(request.permission, { not: { allOf: [ catalogConditions.isEntityKind({ kinds: ['template'] }), { anyOf: [ catalogConditions.hasSpec({ key: 'type', value: 'aws-environment' }), catalogConditions.hasSpec({ key: 'type', value: 'aws-environment-provider' }), ], }, ], }, }); } } ...  ","version":"Next","tagName":"h2"},{"title":"Interacting With Your Application","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/generative-ai/interactingWithApp","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Interacting With Your Application","url":"/docs/techdocs/runtimes/generative-ai/interactingWithApp#prerequisites","content":" Before interacting with your application, you will need to enable certain Bedrock models in your AWS Account.  Navigate to Amazon Bedrock Service in your AWS Console.On the left-hand pane, find Model Access and select Manage model access.Request access for Amazon Titan Embeddings and Anthropic Claude.  ","version":"Next","tagName":"h3"},{"title":"Step 1: Set Configuration Parameters​","type":1,"pageTitle":"Interacting With Your Application","url":"/docs/techdocs/runtimes/generative-ai/interactingWithApp#step-1-set-configuration-parameters","content":" Once the application is successfully deployed, data scientists can set their custom parameters for the solution.  Find the link to your API in the Application's entity page on the OPA website under Links. Use the API Platform of your choice or the Amazon API Gateway Console to make your API request. Be sure to add the /setConfiguration to the URI. Sample request bodies can be found in ./events/set-configuration.json. of the application source code repo. The following parameters can be customized for your solution: response_prompt (string): Used in the Response Lambda, it sets up the context for the Claude model to create an answer. For proper response generation you need to format the prompt using &quot;{question}&quot;, and &lt;document&gt;&lt;/document&gt; XML tags:{documents} . This will serve as placeholders for the questions asked and the documents that will be use to answer it. (More information can be found in Prompt Guidelines) temperature (integer): Used in the Response Lambda, established the amount of randomness injected into the response. Defaults to 1. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and closer to 1 for creative and generative tasks. topics (string): Used in the Classification Lambda, you must pass a list of indices that will reflect those created in OpenSearch. These will also match the high-level folder structure that you created in you S3 Data Bucket. For example: [ “apple”, “google”, “tesla”]. See Step 2: Upload Contextual Documents for more information. max_tokens_to_sample (integer): Used in the Response Lambda, the maximum number of tokens to generate before stopping. We recommend a limit of 4,000 tokens for optimal performance. Note that Anthropic Claude models might stop generating tokens before reaching the set value. chunk_size (integer): Used in Index Lambda, it sets the maximum number of characters in each chunk of text after splitting. This parameter must be set before documents are indexed. If modified after indexing, the documents will need to be re-indexed. chunk_overlap (integer): Used in Index Lambda, determines the amount of overlap between consecutive text chunks, allowing you to control the continuity of context in the generated segments during the splitting process. This parameter must be set before documents are indexed. If modified after indexing, the documents will need to be re-indexed. relevant_document_count (integer): Used in the Retrieval Lambda, it will determine the max number of documents to be retrieved from OpenSearch.  NOTE: Parameter values are configured with default values at the time of resource deployment. All parameters can be used in their default state EXCEPT for topics. This parameter must be configured to meet the requirements specified above and in Step 2: Upload Contextual Documents.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Upload Contextual Documents​","type":1,"pageTitle":"Interacting With Your Application","url":"/docs/techdocs/runtimes/generative-ai/interactingWithApp#context","content":" Once your parameters are set, upload relevant documents to the S3 Data Bucket.  Find the name of your DATA BUCKET. This bucket can be found on the Entity Page of your application, under AWS Infrastructure Resources, listed as an S3 bucket. Create folders inside the bucket corresponding to the names you want your indices to have. This should be something relevant to the documents that will be included in them. For example, a folder labeled “amazon” will contain only documents written about Amazon. These folders will match the list of topics you set in your parameters. Upload your files to their respective folder. This must be a .txt file. Wait for your indices to be created. Adding an object to your bucket will trigger the Indexer Lambda function. You can track the progress of this in the Indexer Lambda’s CloudWatch Logs or verify that the number of indices in OpenSearch increased. Depending on the number and size of the files you upload, this process will take around 1-3 min.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Testing [OPTIONAL]​","type":1,"pageTitle":"Interacting With Your Application","url":"/docs/techdocs/runtimes/generative-ai/interactingWithApp#classification","content":" Once the parameters are set and the documents are uploaded, you are ready to begin asking questions and testing your solution.  NOTE: Each Lambda function is fronted by its own API method. However, the Classification Lambda has the ability to call the Retriever and Response, which will ultimately return to you your final answer with just one API call. You can control this feature through the /classification API call.  If operation_mode =  inclusive, then the Classifier will call the Retriever and Response and return the final message.exclusive, the request will only hit the Classifier and return the message and index fields. Use this for isolated testing.  /classification  { &quot;message&quot;: &quot;Ask your question to the Chatbot here!&quot;, &quot;operation_mode&quot;: &quot;exclusive&quot; }   The response from the /classification will be used as the request for your /retrieval The input will look something like this:  /retrieval  { &quot;message&quot;: &quot;The same question used in the request above will be used here.&quot;, &quot;index&quot;: &quot;An index from your OpenSearch Collection&quot; }   The response from the /retrieval will be used as the request for your /response. The input will look something like this:  /response  { &quot;message&quot;: &quot;The same question used in the request above will be used here.&quot;, &quot;response&quot;: [ { &quot;page_content&quot;: &quot;An extensive string containing relevant document context...&quot; } ] }   The response from the /response will be a string containing the final answer to your initial question.  These events can also be found in the ./events folder of your application's source code repository.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Asking the ChatBot a Question​","type":1,"pageTitle":"Interacting With Your Application","url":"/docs/techdocs/runtimes/generative-ai/interactingWithApp#step-4-asking-the-chatbot-a-question","content":" Now your end users are ready to ask a question. Simply make an API call to the Classfication API method using the request format illustrated in the Step 3: Testing /classification section above. This sample event can also be found in ./events/classification:  /classification  { &quot;message&quot;: &quot;Ask your question to the Chatbot here!&quot;, &quot;operation_mode&quot;: &quot;inclusive&quot; }  ","version":"Next","tagName":"h3"},{"title":"Solution Overview","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/generative-ai/overview","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Solution Overview","url":"/docs/techdocs/runtimes/generative-ai/overview#architecture","content":" The Gen AI Application Software Template will deploy the following resources:      The solution contains 3 main processes:  I. Set Configuration (purple) - sets the parameters to hypertune the model  Application Developer makes API call to the /setConfiguration method in Amazon API Gateway with their specified parameters. Sample request bodies and defined parameters can be found in the ./events folder of your application's source code repository.This will call the Set Configuration Lambda that will take in the updated parameters and store them in AWS SSM Parameter Store to be referenced in other Lambda Functions throughout the solution.  II. Document Upload and Embedding (yellow) - adds context to the LLM  Application Developer uploads contextual files to Amazon S3.Amazon S3 triggers the Indexer Lambda that will call the Amazon Bedrock Titan Embedding Model and store the embeddings of that file in Amazon OpenSearch Service.  III. ChatBot (blue)  End User makes an API request to the /classification method in Amazon API Gateway. Sample request bodies can be found in the ./events folder of your application's source code repository. The Classifier Lambda will take the question passed by the user and extract the keyword that matches the indices created by the Index Lambda. The response of this lambda will be a JSON with the question and the index extracted from it and passed to the Retriever Lambda. The Retriever Lambda takes in the request (user’s questions and index). This index is used to retrieve the relevant document embeddings from Amazon OpenSearch Service Serverless. The response of this lambda will be the user’s questions and all the content of the relevant documents, this will pass as a request for the Response Lambda. The final part of the process is in the Response Lambda, where the request will pass to Amazon Bedrock - Claude LLM. The model will take the content of the documents and the question, analyze them and come up with an answer based on the documents. The final answer is formatted and sent as a response back to the end user. ","version":"Next","tagName":"h2"},{"title":"Cluster Access","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/kubernetes/clusterAccess","content":"","keywords":"","version":"Next"},{"title":"2 Levels of Access​","type":1,"pageTitle":"Cluster Access","url":"/docs/techdocs/runtimes/kubernetes/clusterAccess#2-levels-of-access","content":" OPA makes use of 2 different levels of access when it talks to the kubernetes API server. The first level has cluster-wide administrator access and the second has admin access that is limited to a specific kubernetes namespace.  OPA uses the cluster-wide admin privileges in these scenarios:  Provisioning environment providersApplication CICD pipelines  When you create a new environment provider, OPA will ask you what IAM role you want to use for cluster administration. If you do not provide an existing role, OPA will create one for you. This IAM role is mapped to have full administrative privileges to execute any kubernetes API calls.  OPA uses namespace-limited administrative access to perform lookups and operations on applications. For example, when a developer selects an application in the OPA UI, all reads and writes to Kubernetes that are performed by OPA to support this UI are done using an IAM role that can only see and update kubernetes resources that are assigned to that namespace.  The IAM role that is granted namespace-limited access is determined when a developer sets up an application to run on an OPA environment. OPA will accept an existing IAM role to be provided or it can create the namespace-limited role if desired. It then takes care of updating the aws-auth ConfigMap so that the IAM role is associated with the right kubernetes principal.  ","version":"Next","tagName":"h2"},{"title":"User Access To The Kubernetes API​","type":1,"pageTitle":"Cluster Access","url":"/docs/techdocs/runtimes/kubernetes/clusterAccess#user-access-to-the-kubernetes-api","content":" Users typically interact with the kubernetes API server using kubectl. OPA does not prevent using kubectl in any way. Users can continue to get a kubectl session the same way as they always did.  OPA does allow for one more interesting possibility - it is possible to allow human beings to use the same permissions that OPA does. To enable this, administrators can grant access for users to assume the same AWS IAM role(s) that OPA does. Once they are logged into AWS with a cluster admin or namespace-limited admin role, they can exchange their IAM principal for a token that will allow them to use kubectl.  As prerequisites, users will need to have kubectl and the AWS CLI installed on their machines. Once this is done, they can configure kubectl to talk to their EKS cluster using the following command. Make sure you fill in a real value for the region and cluster name.  aws eks update-kubeconfig --region &lt;my-cluster-region&gt; --name &lt;my-cluster-name&gt;   Another valuable way that OPA IAM roles can be used by humans is for accessing the AWS Web console. By logging into the console as an OPA IAM role, users would be able to go to the EKS service and view their cluster. EKS provides a dashboard that shows many configurations and allows users to see what resources are deployed to their cluster. If users log in with a cluster admin role, they can see everything in the cluster, but if they log in with a namespace-limited role, they will only be able to see the cluster resources that are contained in that namespace. ","version":"Next","tagName":"h2"},{"title":"Import Cluster Template","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/kubernetes/eksProviderTemplates/importClusterTemplate","content":"","keywords":"","version":"Next"},{"title":"How Is This Template Different From The AWS EKS Environment Provider Template?​","type":1,"pageTitle":"Import Cluster Template","url":"/docs/techdocs/runtimes/kubernetes/eksProviderTemplates/importClusterTemplate#how-is-this-template-different-from-the-aws-eks-environment-provider-template","content":" This template performs a subset of what the &quot;AWS EKS Environment Provider&quot; template does. Please see the AWS EKS Environment Provider template documentation for a detailed list of features. The purpose of this template is to use an existing EKS cluster, but to also create the OPA-specific infrastructure that is needed in order for OPA to use the cluster as part of an EKS Environment Provider.  If you use the &quot;AWS EKS Environment Provider From Existing Cluster&quot; template, the following will NOT be created/configured:  VPCEKS ClusterFluentBit log forwardingSecurity GroupsOIDC Provider for service accountsIAM Rolesaws-auth ConfigMap settings  Choosing to use a cluster that is created outside of OPA means that you are responsible for configuring your cluster to be compatible with OPA. See the AWS EKS Environment Provider template documentation for details on what OPA expects to be set up for a cluster.  ","version":"Next","tagName":"h2"},{"title":"Optional Components​","type":1,"pageTitle":"Import Cluster Template","url":"/docs/techdocs/runtimes/kubernetes/eksProviderTemplates/importClusterTemplate#optional-components","content":" The following will only be created if you answer &quot;Yes&quot; to &quot;Create OPA Resources In EKS Cluster?&quot;  &quot;opa-cluster-admin&quot; ClusterRoleBindingClusterRole for viewing/listing namespaces ","version":"Next","tagName":"h3"},{"title":"OPA EKS Architecture","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/kubernetes/architecture","content":"OPA EKS Architecture To understand how OPA Environments and Environment Providers relate to applications and clusters, take a look at the below diagram. This example diagram is meant to show that OPA can work across AWS accounts and regions. OPA allows you to create as many clusters as you want, but also to share the clusters between applications and application environments. The above configuration is just one way that you could set up your apps and clusters. OPA gives you full flexibility on how many accounts, regions, clusters, and environments you want to use as well as which ones are used by which applications. The choice is yours on how many environments you create as well as their scope of use. For example, you can choose to create an Environment for each application or each team, or even for groups of applications. For a more in-depth explanation of OPA environments, see our OPA environments video on YouTube . In the example above, we see an application, &quot;App 1&quot;, that is deployed to 4 distinct Environments. The DEV and TEST Environments are for &quot;non-production&quot; use, and run in a non-prod AWS account. The STAGE and PROD Enviroments run on a production AWS account. The non-prod Environments point to the same Environment Provider, which means that both of these Environments share the same cluster. Developers can make use of kubernetes namespaces to separate application resources (within the same cluster) as they see fit. For example, they could choose to create a namespace per application, a namespace per application environment, or even a namespace per team or department. The STAGE and PROD environments above each use their own separate cluster, which can be on the same AWS account or a different account.","keywords":"","version":"Next"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/kubernetes/gettingStarted","content":"","keywords":"","version":"Next"},{"title":"Creating an EKS Environment Runtime​","type":1,"pageTitle":"Getting Started","url":"/docs/techdocs/runtimes/kubernetes/gettingStarted#creating-an-eks-environment-runtime","content":" Before you can deploy an application to one of your clusters via OPA, you'll need to perform these steps in the OPA/Backstage UI, which are typically performed by someone in the Platform Engineering role:  Create a new EKS Environment Provider by selecting the provider template you want to use and fill out the form to create it. Note that if your template creates a new cluster, it may take 30-60 minutes before the IaC finishes running. Create a new Environment and select the provider that you just created. This establishes an association between an Environment and a Provider. Optionally, you can view the Environment and the Environment Provider from the OPA/Backstage UI. Doing so will show you valuable information about the entity, such as metadata and relationshipts with other entities. The UI will also provide you with a link to the Git repository that contains the configurations and code (if there is any) for the entity you are viewing. info Environment Providers come with their own CICD pipelines that execute their IaC scripts. These pipelines will run automatically to deploy changes if the IaC scripts change.  ","version":"Next","tagName":"h2"},{"title":"Creating a Kubernetes Application​","type":1,"pageTitle":"Getting Started","url":"/docs/techdocs/runtimes/kubernetes/gettingStarted#creating-a-kubernetes-application","content":" Once you have set up an EKS environment via OPA, you or your development team can use the OPA/Backstage UI to choose from one of the EKS application templates. Several template examples are provided out of the box, which you can customize or use as a reference when creating your own application templates. OPA provides example applications written in Java/SpringBoot, NodeJs, and Python. After choosing a template to base your new application on, complete the UI wizard and select to deploy the application to the Environment that was created in the previous steps.  Clicking the button to create an application will trigger a new Git repository to be created, which contains your application code as well as kubernetes manifest files written using either Kustomize or Helm (depending on the application template you selected). The repository will have a CICD pipeline that builds a container image and deploys the application to the selected Environment. After an application has been created, it can be configured to be deployed to additional Environments via the Management tab.  info As of OPA version 0.3.2, all application template examples utilize a traditional CICD pipeline approach and do not follow a GitOps process. This is not due to an OPA limitation, but rather due to the OPA team wanting to release support for kubernetes more quickly. This documentation will be updated if/when a GitOps example becomes available. As always, since OPA is open source, users are free to modify the deployment process to suit their needs. ","version":"Next","tagName":"h2"},{"title":"New Cluster Template","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/kubernetes/eksProviderTemplates/newClusterTemplate","content":"","keywords":"","version":"Next"},{"title":"AWS Infrastructure Created By This Template​","type":1,"pageTitle":"New Cluster Template","url":"/docs/techdocs/runtimes/kubernetes/eksProviderTemplates/newClusterTemplate#aws-infrastructure-created-by-this-template","content":" Standard OPA Provider Components:​  IAM Roles OPA Operations Role Has sufficient permissions to perform operations on the environment provider's resources OPA Provisioning Role Has sufficient permissions to provision the environment provider's resources Audit Table A dedicated DynamoDB table to capture the actions performed on the applications that are running on this environment provider. VPC Created unless an existing VPC ID is provided by the user  EKS-specific Provider Components:​  Security Groups Allow communication between kubernetes worker nodes and the API server EKS ClusterOIDC Provider A cluster-specific OIDC provider that is used for pods to assume IAM rolesMore information on IAM Roles for Service Accounts IAM Roles EKS Cluster Role Role that provides permissions for the Kubernetes control plane to make calls to AWS API operations on your behalf. Cluster Node Role Gives permissions to the kubelet running on the node to make calls to other APIs on your behalf. This includes permissions to access container registries where your application containers are stored. Pod Execution Role (Fargate Only) The pod execution role to use for pods that match the selectors in the Fargate profile. The pod execution role allows Fargate infrastructure to register with your cluster as a node, and it provides read access to Amazon ECR image repositories. Cluster Admin Role Has EKS cluster administration privilegesConfigured in aws-auth ConfigMap so that it has k8s cluster admin privilegesUsed by CICD during provisioning provider resourcesCan be assumed by administrators as well if its trust policy is customized EC2 Instances Provisioned for use as cluster nodesOnly created if the user chose to use Managed Nodes instead of Fargate for the cluster Kubectl Lambda Function A lambda function used for configuring and querying kubernetes cluster resources, including installing Helm chartsThis function runs in the same VPC as the EKS cluster so that it can communicate with the kubernetes API server even if the API server is not exposed publiclyUsed by Infrastructure as Code (IaC) that adds/updates resources on the EKS cluster For example, OPA's EKS Environment Provider templates run CDK code to configure cluster resources OPA uses this lambda function to update and query the EKS cluster instead of using direct calls to the API serverThis function makes use of kubectl (by way of a lambda layer). The version of kubectl it uses must be compatible with the kubernetes cluster version. If you update your kubernetes cluster version, you should also update the kubectl lambda layer so that the kubectl client version matches your cluster.  OPA Uses of the Kubectl Lambda Function:​      Kubernetes Components Created/Configured By This Template​  AWS Load Balancer Controller A controller that will create AWS Load Balancers based on Kubernetes configurations FluentBit Configured to forward pod logs to AWS Cloudwatch logs info The OPA UI has a Logs tab that can show an application's logs from a particular environment. If the cluster is not set up to have FluentBit (or a similar tool such as Fluentd) send application logs to CloudWatch, this functionality will not work. ClusterRoleBinding This binds the principal that OPA uses to administer the cluster (&quot;opa-cluster-admin&quot;) to the ClusterRole that has cluster-wide admin permissions ClusterRole for viewing/listing namespaces OPA application operations make use of this ClusterRole aws-auth ConfigMap settings The aws-auth ConfigMap is updated to configure a k8s principal for OPA to use to perform cluster and application provisioning ","version":"Next","tagName":"h2"},{"title":"Entities","type":0,"sectionRef":"#","url":"/docs/techdocs/entities","content":"","keywords":"","version":"Next"},{"title":"AWS Environment & Environment Provider​","type":1,"pageTitle":"Entities","url":"/docs/techdocs/entities#aws-environment--environment-provider","content":" These custom Backstage entity kinds were created to represent an abstract AWS environment and environment provider.  ","version":"Next","tagName":"h2"},{"title":"AWS Environment​","type":1,"pageTitle":"Entities","url":"/docs/techdocs/entities#aws-environment","content":" An abstracted entity that captures the intent of use of a particular environment instance, including:  Single/multi account.Single/multi region.Category – dev, test, stage, prod etc.Classification – private, internal, public.Requires approval - for automated pipeline deployments.System - for participation in higher level systems.Hierarchy – where does it position in the hierarchy of other environments (low – dev, high – prod).  info AWS Environment Entity Definition: AWSEnvironmentEntityV1.ts  AWS Environment Principles:  Maintain 1:N relationship with AWS environment providers.Integrated with pipeline definitions for deployments that require approval.Customizable and extendable.Can be used with different types of Environment Providers.  AWSEnvironmentEntityV1.ts export interface AWSEnvironmentEntityV1 extends Entity { apiVersion: 'aws.backstage.io/v1alpha'; kind: 'AWSEnvironment'; spec: { ... } }   AWS Environment enforces the creation of a relationship with environment providers through the entity Processor  AWSEnvironmentEntitiesProcessor.ts if (targetRef.kind == 'awsenvironmentprovider') { emit( processingResult.relation({ source: selfRef, type: RELATION_DEPENDS_ON, target: { kind: targetRef.kind, namespace: targetRef.namespace, name: targetRef.name, }, }), ); emit( processingResult.relation({ source: { kind: targetRef.kind, namespace: targetRef.namespace, name: targetRef.name, }, type: RELATION_DEPENDENCY_OF, target: selfRef, }), ); }   info AWS Environment Processor code: AWSEnvironmentEntitiesProcessor.ts  ","version":"Next","tagName":"h3"},{"title":"AWS Environment Provider​","type":1,"pageTitle":"Entities","url":"/docs/techdocs/entities#aws-environment-provider","content":" A custom kind entity that captures a place in the cloud that can be used to provision and run applications. An Environment provider can be defined by:  A particular AWS AccountA particular AWS RegionA name and prefix composition for organization segmentation (payments:development, hr:production etc.)Mutually exclusive – multiple distinct providers can be created within a single AWS account and regionIsolated from other providers / accountsProvisioning role - a role that has sufficient permissions to provision the resources for the designated types of applications.Operations role - a role that has sufficient permissions to operate the designated types of applications.Audit table - a dedicated table to capture the actions performed on the applications running in the current environment.Optional: The underlying networking (VPC), runtime environment (ECS/EKS/Serverless), and required applications infrastructure      info AWS Environment Provider Entity Definition: AWSEnvironmentProviderEntityV1.ts  AWSEnvironmentEntityV1.ts export interface AWSEnvironmentProviderEntityV1 extends Entity { apiVersion: 'aws.backstage.io/v1alpha'; kind: 'AWSEnvironmentProvider'; spec: { ... } }   info AWS Environment Provider Processor code: AWSEnvironmentProviderEntitiesProcessor.ts  ","version":"Next","tagName":"h3"},{"title":"Components​","type":1,"pageTitle":"Entities","url":"/docs/techdocs/entities#components","content":" We map applications to the existing Backstage entity component. While the concept of an application can be interpreted in different ways, we found the kind component to be very close to it.  ","version":"Next","tagName":"h2"},{"title":"The structure of application component entity:​","type":1,"pageTitle":"Entities","url":"/docs/techdocs/entities#the-structure-of-application-component-entity","content":" When provisioning an application, the template creates a Backstage catalog info yaml file with the below properties:  apiVersion: backstage.io/v1alpha1 kind: Component metadata: name: &quot;Your App name&quot; description: &quot;Description&quot; tags: - aws - nodejs iacType: cdk repoSecretArn: &quot;arn:aws:secretsmanager:us-east-1:**********.:secret:***&quot; spec: type: aws-app owner: &quot;group:default/developers&quot; lifecycle: experimental dependsOn: [FirstDeployedEnvironment]   We introduce a new component spec type - aws-app which will be used to mark applications that run on AWS. This is used to provide a specific UI experience that allows users to operate the application in the AWS cloud.  The iacType property indicates the type of the infrastructure as code this app was provisioned with - this impacts both the pipeline as well as the UI experience that are based on Terraform or CDK (state management).  tip You may notice that repoSecretArn is created regardless of the environment where the application is deployed - that is because the access to the repository is part of the platform / solution account regardless of where is it being deployed.  After the application provisioning pipeline completes, the pipeline will update the entity with the environment deployed resources under the appData tag:  apiVersion: backstage.io/v1alpha1 kind: Component metadata: name: &quot;AML-Detection-EMEA&quot; description: &quot;AML Detection App for EMEA&quot; tags: - aws - nodejs annotations: aws.amazon.com/opa-repoSecretArn: &quot;arn:aws:secretsmanager:us-east-1:**********.:secret:aws-apps-aml-detection-emea-access-token-V9w8Ea&quot; iacType: cdk repoSecretArn: &quot;arn:aws:secretsmanager:us-east-1:**********.:secret:aws-apps-aml-detection-emea-access-token-V9w8Ea&quot; appData: EMEA-AML-dev: emea-aml-dev: EcrRepositoryUri: **********.dkr.ecr.eu-west-1.amazonaws.com/aml-detection-emea-emea-aml-dev EcrRepositoryArn: arn:aws:ecr:eu-west-1:**********.:repository/aml-detection-emea-emea-aml-dev EcsServiceArn: arn:aws:ecs:eu-west-1:**********.:service/aml-emea-aml-dev-cluster/AML-Detection-EMEA-emea-aml-dev EcsTaskDefinitionArn: arn:aws:ecs:eu-west-1:**********.:task-definition/******:1 AlbEndpoint: http://AML-D-AMLDe-11C7BFBPF0RHP-**********..eu-west-1.elb.amazonaws.com TaskLogGroup: /aws/apps/EMEA-AML-dev/emea-aml-dev/AML-Detection-EMEA TaskExecutionRoleArn: AML-Detection-EMEA-ecs-resources-emea-aml-dev/AML-Detection-EMEA-taskDef/ExecutionRole AppResourceGroup: arn:aws:resource-groups:eu-west-1:**********.:group/AML-Detection-EMEA-emea-aml-dev-rg StackName: AML-Detection-EMEA-ecs-resources-emea-aml-dev EMEA-AML-test: emea-aml-test: EcrRepositoryUri: **********..dkr.ecr.eu-west-1.amazonaws.com/aml-detection-emea-emea-aml-test EcrRepositoryArn: arn:aws:ecr:eu-west-1:**********.:repository/aml-detection-emea-emea-aml-test EcsServiceArn: arn:aws:ecs:eu-west-1:**********.:service/aml-emea-aml-test-cluster/AML-Detection-EMEA-emea-aml-test EcsTaskDefinitionArn: arn:aws:ecs:eu-west-1:**********.:task-definition/*****:1 AlbEndpoint: http://AML-D-AMLDe-HNBIDJ2F0QER-**********.eu-west-1.elb.amazonaws.com TaskLogGroup: /aws/apps/EMEA-AML-test/emea-aml-test/AML-Detection-EMEA TaskExecutionRoleArn: AML-Detection-EMEA-ecs-resources-emea-aml-test/AML-Detection-EMEA-taskDef/ExecutionRole AppResourceGroup: arn:aws:resource-groups:eu-west-1:**********.:group/AML-Detection-EMEA-emea-aml-test-rg StackName: AML-Detection-EMEA-ecs-resources-emea-aml-test spec: type: aws-app owner: &quot;group:default/developers&quot; lifecycle: experimental dependsOn: ['awsenvironment:default/EMEA-AML-dev', 'awsenvironment:default/EMEA-AML-test']   tip You may notice that the example above describes an entity of an application that is deployed in two environments: EMEA-AML-dev and EMEA-AML-test  Similiarly a serverless application entity will look like :  apiVersion: backstage.io/v1alpha1 kind: Component metadata: name: &quot;snacks&quot; kebabName: &quot;snacks&quot; description: &quot;get a list of yummy snack foods&quot; tags: - aws - rest - swagger - openapi - apigateway - serverless iacType: cdk repoSecretArn: &quot;arn:aws:secretsmanager:us-east-1:**********:secret:aws-apps-snacks-access-token-0IMiXR&quot; appData: api-team-dev: api-team-dev: AppResourceGroup: arn:aws:resource-groups:us-east-1:**********::group/snacks-api-team-dev-rg StackName: snacks-serverless-api-resources-api-team-dev AppStackName: snacks-api-team-dev BuildBucketName: snacks-serverless-api-re-snacksapiteamdevbuildbuc-********** api-team-qa: api-team-qa: AppResourceGroup: arn:aws:resource-groups:us-east-1:**********:group/snacks-api-team-qa-rg StackName: snacks-serverless-api-resources-api-team-qa AppStackName: snacks-api-team-qa BuildBucketName: snacks-serverless-api-re-snacksapiteamqabuildbuck-********** spec: type: aws-app owner: &quot;group:default/developers&quot; lifecycle: experimental dependsOn: ['awsenvironment:default/api-team-dev', 'awsenvironment:default/api-team-qa']   ","version":"Next","tagName":"h3"},{"title":"Resource​","type":1,"pageTitle":"Entities","url":"/docs/techdocs/entities#resource","content":" Backstage resource entities are used to illustrate AWS resources. However, we extended the model so that we can articulate specific UI and processes that won't conflict with Backstage built-in capabilities of Resource entities.  Resource entity catalog:  apiVersion: backstage.io/v1alpha1 kind: Resource metadata: name: &quot;Commercial-Pymt-db-us&quot; description: &quot;Payment DB for commercial apps in US&quot; tags: - aws - rds - aws-resource - database annotations: null iacType: cdk resourceType: &quot;aws-rds&quot; dbName: &quot;pymtdb&quot; dbObjectName: &quot;user&quot; appData: US-Commercial: us-commercial-dev: Arn: arn:aws:rds:us-east-1:**********:db:commercial-pymt-db-us-rds-reso-rdsinstance05f4b4b0-********** DbAdminSecretArn: arn:aws:secretsmanager:us-east-1:**********:secret:rdsInstanceSecretB79B16A5-**********-Y8ykm1 DbEndpoint: commercial-pymt-db-us-rds-reso-rdsinstance05f4b4b0-xwcil8gg8rvp.**********.us-east-1.rds.amazonaws.com DbPort: '5432' ResourceGroup: arn:aws:resource-groups:us-east-1:**********:group/Commercial-Pymt-db-us-us-commercial-dev-rg StackName: Commercial-Pymt-db-us-rds-resource spec: type: aws-resource owner: &quot;group:default/dev-ops&quot; lifecycle: experimental dependsOn: ['awsenvironment:default/US-Commercial']   Core resource entity properties:  spec/type -&gt; aws-resource . while we use the same kind of Resource the spec type refers to aws-resource which can be easily used to filter our AWS related resources.resourceType this is used to capture the different types of AWS resources including: aws-rds, s3, sns, sqs etc.  Extended resource entity properties:  dbName - in this aws-rds resource the property captures the database namedbObjectName in this aws-rds resource the property captures the schema/user nameappData - under this property, all deployed resource artifacts are preserved - you can customize what properties you would like to bring back to the entity from the provisioning pipeline.  note AWS Resources are provisioned against a single environment. The process of deploying a resource to an additional environment is not supported to avoid managing different configurations or changes of the resource that are not identical across multiple environments. ","version":"Next","tagName":"h2"},{"title":"Kubernetes Applications","type":0,"sectionRef":"#","url":"/docs/techdocs/runtimes/kubernetes/k8sApps","content":"","keywords":"","version":"Next"},{"title":"Components of an OPA Kubernetes Application​","type":1,"pageTitle":"Kubernetes Applications","url":"/docs/techdocs/runtimes/kubernetes/k8sApps#components-of-an-opa-kubernetes-application","content":" No matter which application template a developer chooses, a Git repository will be created for their application that contains the following:  Backstage entity YAML file This is automatically read by OPA so that your applications show up in the portal Application source codeKubernetes manifests (utilizing Helm or Kustomize for multi-environment deployment support)A CICD pipeline that will build your application's container image and deploy it to each of your application's configured environmentsInfrastructure as Code (IaC) scripts that provision application-specific infrastructure  ","version":"Next","tagName":"h2"},{"title":"FAQ​","type":1,"pageTitle":"Kubernetes Applications","url":"/docs/techdocs/runtimes/kubernetes/k8sApps#faq","content":" Q: How Does OPA CICD support deploying my application using Helm or Kustomize? A: The CICD pipeline detects whether the application is set up using Helm or Kustomize. It then resolves these configurations for each environment to create pure kubernetes manifests. The deployment step uses the pure kubernetes manifests, not the Helm or Kustomize configurations. The generated manifests are stored in the Git repository by OPA. The manifests are stored in JSON and YAML format. The file names are next-release.json and next-release.yaml, respectively. Q: How Does OPA CICD handle deploying my kubernetes application to more than one environment? A: For each environment your application is deployed to, OPA will create a separate folder for environment-specific kubernetes manifest customizations. If your application is configured with Helm, the separate folder only contains a values.yaml file that has settings for the environment. If your application is configured with Kustomize, the separate folder will contain overlay files that customize your base manifests. Q: How Does OPA UI Get The State of My Application's Deployments? A: OPA will query the cluster. The query will limit results to the kubernetes namespace that the application uses in a particular deployment environment. The query will also limit results to Deployments that have a &quot;app.kubernetes.io/name&quot; label set to the application's name and a &quot;app.kubernetes.io/env&quot; label set to the environment that the application is running in. ","version":"Next","tagName":"h2"},{"title":"Security","type":0,"sectionRef":"#","url":"/docs/techdocs/security","content":"","keywords":"","version":"Next"},{"title":"Authentication​","type":1,"pageTitle":"Security","url":"/docs/techdocs/security#authentication","content":" The Backstage Authentication system provides for sign-in and identification of users. The OPA on AWS reference implementation is pre-configured to use Okta as an authentication provider. This demonstrates one type of authentication provider; however, it can easily be changed to other common identity providers such as Active Directory and other OAuth 2 supported providers.  info Multiple providers are supported by Backstage. For the complete list, consult the Backstage &quot;Included providers&quot; documentation  ","version":"Next","tagName":"h2"},{"title":"Organizational data​","type":1,"pageTitle":"Security","url":"/docs/techdocs/security#organizational-data","content":" Backstage provides support to map organizational data such as users and groups into entities in the Backstage catalog. The OPA on AWS reference implementation is pre-configured to use the OktaOrgEntityProvider from the catalog-backend-module-okta plugin from Roadie. This plugin will read Okta Users and Groups from a configured Okta instance and create entities in the catalog. Once they are populated in the catalog, user and group entities can be used for creating relationships (e.g. &quot;ownedBy&quot;) with other entities. User and Group entities can also be used in the Backstage permission framework to enforce allow/deny policy decisions.  Alternatives for organizational data ingestion can be configured for OpenLDAP, GitHub, GitLab, and more. Consult the Backstage Integrations documentation and Backstage plugin directory for additional options and details.  ","version":"Next","tagName":"h2"},{"title":"AWS Security​","type":1,"pageTitle":"Security","url":"/docs/techdocs/security#aws-security","content":" ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Security","url":"/docs/techdocs/security#introduction","content":" While there are different methodologies when it comes to security in the cloud, we adopted some of the approaches we have seen at our financial customers. These include:  Segregation of access controlsUse of temporary credentialsSeparation of duties and the least privileges principleUse of different AWS accounts to ensure default restrictionsSeparation of production/staging environments from development/sandbox environments  note The above principles are not a replacement for organization-level preventive and detective controls. It is recommended to use services like AWS Control Tower and Service Control Policies (SCPs) to have multiple layers of security from the developer platform throughout the organization governance and controls.  ","version":"Next","tagName":"h3"},{"title":"Environments and access to AWS services​","type":1,"pageTitle":"Security","url":"/docs/techdocs/security#environments-and-access-to-aws-services","content":" We define an environment as a &quot;place&quot; in AWS. Granting access to an environment will allow a user or process to gain access only to the resources of that environment, but not to other environments.  Definition of AWS Environment Provider:  Include all of the supporting software artifacts to run a particular type of workload (Container-based, Serverless, AI/ML etc..).Must reside within an AWS Account and a region.Must provide at least two roles to gain access to the environment resources: a Provisioning role and an Operations role.Mutually exclusive by design - One provider cannot access resources from another provider, even if they are configured to use the same AWS account and region.  There are several methods for a user to gain access to an environment:  Indirect access - throughout the developer platform and pipeline, a developer can perform actions on an allowed target environment. The platform/pipeline will assume the target environment roles and get temporary credentials to perform the actions on behalf of the user.      Direct access - a user can assume credentials directly to the target environment access roles (Provisioning roles, operations role)    Resource specific access - a user can assume access to a particular shared resource by the environment, such as: kubectl    info Environment provider authors can reason about the required set of permissions for provisioning and operating applications for this environment. OPA on AWS provides sample templates for Amazon ECS, Amazon EKS, and Serverless environment providers.  ","version":"Next","tagName":"h3"},{"title":"Diving deep to access roles trust policy​","type":1,"pageTitle":"Security","url":"/docs/techdocs/security#diving-deep-to-access-roles-trust-policy","content":" As described above, OPA on AWS provides a pattern of assuming access to particular AWS environments. Out of the box, when provisioning a new environment provider, the provisioning role and operations role are created and their trust policy is modified to enable the platform role and pipeline role respectively.  example of: commercial-us-commercial-dev-provisioning-role - Assumed by gitlab pipeline role  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::123456789012:role/opa-platform-GitlabRunnerConstructGitlabRunnerIamR&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] }   example of: commercial-us-commercial-dev-operations-role - Assumed by the platform role  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::123456789012:role/backstage-master-role&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] }   tip You can also configure these roles with federation to grant access to your users based on their membership in certain group that is set up for your identity provider. This can be done with SAML or WebIdentity - for more information, read here. ","version":"Next","tagName":"h3"},{"title":"Application Environment Tutorial","type":0,"sectionRef":"#","url":"/docs/tutorials/create-environments","content":"","keywords":"","version":"Next"},{"title":"Create an Environment Provider​","type":1,"pageTitle":"Application Environment Tutorial","url":"/docs/tutorials/create-environments#create-an-environment-provider","content":" We're going to start with creating a new AWS Environment Provider.  In the default OPA on AWS solution used by this tutorial, the Infrastructure as Code source and CI/CD pipelines responsible for provisioning infrastructure have already been created and reside in a GitLab reference repository.  We'll use an AWS ECS Environment Provider template to provision our AWS Elastic Container Service (ECS) runtime environment.  1. Click the Create.. menu on the left and Choose the AWS ECS Environment Provider template.    2. Provide input parameters to tailor the environment provider resources.  Parameter\tValue\tDescriptionName\tecs-dev-provider\tA unique identifier for the environment provider Prefix\tecsdev\tA short prefix used for AWS resource creation Description\tA Dev environment provider for containerized applications\tDescribes what the environment provider's purpose will be Owner\tSelect a group from the drop-down list to own the Environment Provider\tThe group or user that will be responsible for the entity AWS Account Number\tEnter your 12-digit AWS account id\tThe AWS account where the environment provider will be created AWS Region\tSelect the same region where you have deployed the OPA on AWS solution\tThe AWS region to provision resources in Environment role arn\tEnter the ARN of an IAM role with sufficient permission to deploy AWS infrastructure. A sample role named opa-envprovisioning-role was created when deploying the OPA on AWS solution. The ARN format should be similar to arn:aws:iam::{AWS_ACCOUNT_ID}:role/{IAM_ROLE_NAME}\tThe IAM role to be used for provisioning AWS resources in the target account and region CIDR\t10.0.0.0/24 (default)\tThe CIDR block to be provisioned for the VPC associated with the ECS cluster to be created    3. Select Next Step and Provide Repository information  In this step we provide information about the git repository where the Environment Provider source files will be published as part of executing the Software Template.  Parameter\tValue\tDescriptionHost:\tdefault value\tThe GitLab host name Owner:\tdefault value\tThe GitLab namespace where this repository will belong to. It can be the name of organization, group, subgroup, user, or the project Repository:\tecs-dev-provider\tThe name for the git repository    4. Select Next Step, review your inputs and select Create.    5. Environment Provider Entity Creation  Upon clicking Create, the template starts executing automated action steps. It gets info from AWS, fetches ECS infrastructure code and other configuration files from the reference repository, updates configuration inside these files, and sets up a new git repository to store the files.    Once this git folder is ready, a CI/CD process will start automatically. It will use the infrastructure code in the git repository to provision your new ECS platform.  Now, let's proceed to the next step to create an environment using the environment provider entity we just created.  ","version":"Next","tagName":"h2"},{"title":"Create an Environment​","type":1,"pageTitle":"Application Environment Tutorial","url":"/docs/tutorials/create-environments#create-an-environment","content":" To create an Environment Entity we will use the AWS Environment template from the software catalog section.  1. On the OPA on AWS website, navigate to the Create.. menu. From the list of available templates, choose the AWS Environment template card.    2. Provide input parameters to tailor the environment entity and click Next Step  Parameter\tValue\tDescriptionName\tecs-dev\tA unique identifier for the environment Short Name\tecsdev\tA short identifier used for identification of environments Description\tDevelopment environment for containerized applications deployed to ECS\tDescribes the purpose of the environment Environment Type\tAWS ECS\tSpecifies the type of applications that can be deployed and managed in this environment Deployment Requires Approval\tNo\tAllows for blocking a CI/CD pipeline until approval is received. This is often set to 'No' for development and test, but 'Yes' for production Owner\tSelect a group from the drop-down list to own the Environment\tThe group or user that will be responsible for the entity. Account Type\tSingle-account\tIndicates whether the environment supports one or more accounts Region Type\tSingle-region\tIndicates whether the environment supports one or more regions Category\tDevelopment\tIdentifies the intended purpose of the environment (dev, test, prod, etc) Classification\tpublic\tIdentifies visibility of the environment (public-facing, internal-facing, or private) System\tdemo\tIdentifies the relationship of this environment to a larger system. This can be used to group environments together. Hierarchy of the Environment\t1\tValue used to determine relative ordering for environments. Typical approaches are to use lower numbers for development and higher numbers for production. Providers\tecs-dev-provider\tSelects one or more environment providers to attach to this environment. In this tutorial, there may only one environment provider available, so the drop-down list is disabled and the available environment provider will be used.    2. Click Next Step and fill in the remaining information:  Parameter\tValue\tDescriptionHost\tdefault value\tThe GitLab host name Owner\tdefault value\tThe GitLab namespace where this repository will belong to. It can be the name of organization, group, subgroup, user, or the project Repository\tecs-dev\tThe name for the git repository    3. Click Next Step and review your inputs. Click Create    4. The template scaffolding action will fetch a catalog-info.yaml file (used to define entities), replace the placeholders with our input we just provided, push the file to a new repo, and register it to the Backstage catalog.    Now that our Environment Entity is created, our developers can discover and point to the it during their app scaffolding process. Let's proceed to the next step and explore the ECS Environment Entity we just created. ","version":"Next","tagName":"h2"},{"title":"Create Resources","type":0,"sectionRef":"#","url":"/docs/tutorials/create-resources","content":"","keywords":"","version":"Next"},{"title":"Creating an RDS Database resource​","type":1,"pageTitle":"Create Resources","url":"/docs/tutorials/create-resources#creating-an-rds-database-resource","content":" 1. In the OPA on AWS interface, select the Create... menu option in the left navigation. Search for and choose the AWS RDS Database template.    2. Provide information about the resource and select Next Step  Parameter\tValue\tDescriptionName\tdemo-db\tA unique identifier for the database Description\tA database for demonstration data\tDescribes the database's purpose Owner\tSelect a group from the drop-down list to own the Resource\tThe group or user that will be responsible for the entity    3. Next, provide deployment information for the resource and select Next Step  Parameter\tValue\tDescriptionAWS Environment\tthe ecs-dev environment will be pre-selected since it is the only environment available\tThe AWS Environment in which you want to deploy your application to. The environment that we created in a previous tutorial will already be selected for you    4. Provide database configuration information and select Next Step  Parameter\tValue\tDescriptionDatabase Name\tdemodb\tThe name of a default database to create in the RDS instance Object Name\tuser\tThe name of the object that you will track in the default database. Database Engine\tSelect PostgreSQL\tThe engine of the SQL database Database Size\tSelect Small (20GB)\tThe size of the database that will be deployed    5. Provide Repository information and select Next StepChoose a repository for this database's infrastructure source code and entity information - you can use demo-db  6. Create the database by clicking the Create button    The template scaffolding action will fetch a catalog-info.yaml file (used to define entities), replace the placeholders with our input we just provided, push the file to a new repo, and register it to the Backstage catalog. A CI/CD pipeline in our new repository will begin executing to provision the new database. You can monitor the progress of the pipeline in the &quot;CI/CD&quot; tab of the new resource entity. ","version":"Next","tagName":"h2"},{"title":"Create Apps","type":0,"sectionRef":"#","url":"/docs/tutorials/create-app","content":"Create Apps In this tutorial, you will build out a new AWS microservice application and deploy it as a into an Environment. By deploying into an Environment designed to run containerized applications in AWS Elastic Container Service (ECS), the application will be automatically packaged and deployed for you as an ECS task in an ECS cluster. info This tutorial assumes that you have run the Create an Environment tutorial and expects input values from that setup. If you have created different Environment entities, then substitute the appropriate values. 1. In the OPA on AWS site, navigate to the Create... menu. Search for and select Node.js Express Web App. 2. Provide application input parameters and select Next Step Parameter\tValue\tDescriptionName\tdemo-app\tA unique identifier for the application Description\tNode.js demo application running on ECS\tDescriptive information about the application Owner\tSelect a group from the drop-down list to own the application\tThe group or user that will be responsible for the entity AWS Environment\tthe ecs-dev environment will be pre-selected since it is the only environment available\tThe AWS Environment in which you want to deploy your application to. The environment that we created in the previous step will already be selected for you 3. Provide Repository information and select Next Step Parameter\tValue\tDescriptionHost\tdefault value\tThe GitLab host name Owner Available\tdefault value\tThe GitLab namespace where this repository will belong to. It can be the name of organization, group, subgroup, user, or the project Repository\tdemo-app\tThe name for the git repository 4. Review your inputs and select Create Your application will begin to deploy. A Task Activity view will show you the progress of all of the actions executed to build out your application. This includes gathering environment information, creating secrets, scaffolding a new repository, creating an access token for the repo, and registering the entity with backstage.","keywords":"","version":"Next"},{"title":"Test Cases","type":0,"sectionRef":"#","url":"/docs/tests","content":"","keywords":"","version":"Next"},{"title":"Assumptions​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#assumptions","content":" You have OPA on AWS platform deployed and running.You have several accounts that are used or can be used to provision environments and applications.You have an identity provider configured with backstage(Okta, AD, etc.)  ","version":"Next","tagName":"h2"},{"title":"Test case, context and expected result​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#test-case-context-and-expected-result","content":" We define a context of a test with the letter &quot;C&quot;. This will allow us to describe the existing state before we can execute a test case.  We also define test case with the letter &quot;T&quot; which describe the set of instructions to test a scenario on OPA on AWS.  Lastly we define the expected result with the letter &quot;E&quot;, this will describe the expected assertion of a resulted test case.  ","version":"Next","tagName":"h2"},{"title":"Context​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#context","content":" ID\tDescriptionC01\tAn entitled user logged-in to backstage C02\tA provisioned ECS Environment(Single provider) is available C03\tA provisioned EKS Environment(Single provider) is available C04\tA provisioned Serverless Environment(Single provider) is available C05\tA provisioned AWS Environment is available C06\tA provisioned ECS application(Single environment) is available C07\tA provisioned EKS application(Single environment) is available C08\tA provisioned Serverless application(Single environment) is available C09\tA provisioned shared RDS Resource is available C10\tA provisioned shared S3 Resource is available C11\tA binding between an application and resource exist C12\tAn application is deployed to more than one environment C13\tA provisioned ECS Environment(Multi provider) is available C14\tA provisioned EKS Environment(Multi provider) is available C15\tA provisioned Serverless Environment(Multi provider) is available C16\tA provisioned ECS application(Multi environment) is available C17\tA provisioned EKS application(Multi environment) is available C18\tA provisioned Serverless application(Multi environment) is available C19\tA provisioned Terraform application/provider/resource C20\tUser chooses that a new VPC should be created C21\tUser chooses that an existing VPC ID should be used instead of creating a new VPC C22\tA provisioned EKS Cluster is available C23\tA provisioned EKS Cluster Kubectl Lambda is available  ","version":"Next","tagName":"h2"},{"title":"Expected result​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#expected-result","content":" ID\tDescriptionE01\tSuccessfully provision an ECS provider E02\tSuccessfully provision an EKS provider E03\tSuccessfully provision an Serverless provider E04\tSuccessfully provision an Environment E05\tSuccessfully provision an ECS application(Single provider) E06\tSuccessfully provision an EKS application(Single provider) E07\tSuccessfully provision a Serverless application(Single provider) E08\tSuccessfully start and stop an application E09\tSuccessfully view application logs E10\tSuccessfully view application audit table E11\tSuccessfully set environment variables to an application E12\tSuccessfully view cloud related application resources E13\tSuccessfully bind a shared resource to an application E14\tSuccessfully deploy an application to additional environments E15\tSuccessfully toggle between deployed environments of an application E16\tSuccessfully delete an application from an environment E17\tSuccessfully delete an application from all environments E18\tSuccessfully delete a shared resource E19\tSuccessfully delete a provider E20\tSuccessfully delete an environment E21\tSuccessfully view provider details E22\tSuccessfully unbind a shared resource to an application E23\tA new VPC is created for the provider E24\tAn existing VPC ID is used by the provider E25\tSuccessfully provision an RDS database as a shared resource E26\tSuccessfully provision an S3 bucket as a shared resource E27\tSuccessfully create EKS provider using existing EKS cluster and create new kubectl lambda E28\tSuccessfully create EKS provider using existing EKS cluster and existing kubectl lambda  ","version":"Next","tagName":"h2"},{"title":"Test cases​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#test-cases-1","content":" ","version":"Next","tagName":"h2"},{"title":"Platform engineering​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#platform-engineering","content":" Environment Providers​  ID\tContext\tDescription\tExpected ResultT001\tC01+C20\tProvision an ECS environment provider with serverless compute (Fargate)\tE01+E23 T002\tC01+C21\tProvision an ECS environment provider with serverless compute (Fargate)\tE01+E24 T003\tC01+C20\tProvision an ECS environment provider with EC2 compute\tE01+E23 T004\tC01+C20\tProvision an EKS environment provider with serverless compute (Fargate)\tE02+E23 T005\tC01+C21\tProvision an EKS environment provider with serverless compute (Fargate)\tE02+E24 T006\tC01\tProvision an EKS environment provider with managed nodes\tE02 T007\tC01\tProvision an EKS environment provider with private-only API access\tE02 T008\tC01+C20\tProvision a Serverless environment provider\tE03+E23 T009\tC01+C21\tProvision a Serverless environment provider\tE03+E24 T010\tC01+(C02,C03,C04)\tView provider information card - on provider page\tE21 T011\tC01+(C02,C03,C04)\tDelete provider - on provider management page\tE19 T012\tC01+C22\tProvision an EKS environment provider and import an existing cluster and create kubectl lambda\tE27 T013\tC01+C22+C23\tProvision an EKS environment provider and import an existing cluster and reuse existing kubectl lambda\tE28  Environments​  ID\tContext\tDescription\tExpected ResultT200\tC01\tProvision an AWS environment with a single provider\tE04 T201\tC01+C05\tView environment information card - on environment page\tE21 T202\tC01+C05\tDelete environment - on environment management page\tE21 T203\tC01+C05\tAdd additional provider to an existing environment - on environment page\tE19 T204\tC01+C05\tDelete a provider from an existing environment - on environment page\tE19  ","version":"Next","tagName":"h3"},{"title":"Developer​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#developer","content":" Application Provisioning​  ID\tContext\tDescription\tExpected ResultT400\tC01 + C02\tProvision a node.js application on Amazon ECS environment with a single provider\tE05 T401\tC01 + C02\tProvision a Java SpringBoot application on Amazon ECS environment with a single provider\tE05 T402\tC01 + C02\tProvision a python application on Amazon ECS environment with a single provider\tE05 T403\tC01 + C02\tProvision a node.js Terraform application on Amazon ECS environment with a single provider\tE05 T404\tC01 + C03\tProvision a node.js application on Amazon EKS environment with a single provider\tE06 T405\tC01 + C03\tProvision a Java SpringBoot application on Amazon EKS environment with a single provider\tE06 T406\tC01 + C03\tProvision a python application on Amazon EKS environment with a single provider\tE06 T407\tC01 + C03\tProvision a node.js Terraform application on Amazon EKS environment with a single provider\tE06 T408\tC01 + C04\tProvision a Serverless REST API application on AWS Serverless environment with a single provider\tE07  Application operations​  ID\tContext\tDescription\tExpected ResultT600\tC06\tStart and Stop the application\tE08 T601\tC06\tView application cloudwatch logs - on App logs tab\tE09 T602\tC06\tView application audit table - on audit tab\tE10 T603\tC06\tAdd/edit/delete environment variables for the application\tE11 T604\tC06\tView related cloud resources for the application\tE12 T605\tC06 + C02\tDeploy application to another available environment\tE14 T606\tC06 + C12\tToggle between deployed environments for the application\tE15 T607\tC06\tDelete an application from an environment\tE16 T608\tC06 + C12\tDelete an application from all environments\tE17 T609\tC06 + C09\tBind a shared resource RDS to an application\tE13 T610\tC06 + C09 + C11\tUnbind a shared resource from an application\tE22 T611\tC07\tStart and Stop the application\tE08 T612\tC07\tView application cloudwatch logs - on App logs tab\tE09 T613\tC07\tView application audit table - on audit tab\tE10 T614\tC07\tAdd/edit/delete environment variables for the application\tE11 T615\tC07\tView related cloud resources for the application\tE12 T616\tC07 + C02\tDeploy application to another available environment\tE14 T617\tC07 + C12\tToggle between deployed environments for the application\tE15 T618\tC07\tDelete an application from an environment\tE16 T619\tC07 + C12\tDelete an application from all environments\tE17 T620\tC07 + C09\tBind a shared resource RDS to an application\tE13 T621\tC07 + C09 + C11\tUnbind a shared resource from an application\tE22  ","version":"Next","tagName":"h3"},{"title":"Shared resources​","type":1,"pageTitle":"Test Cases","url":"/docs/tests#shared-resources","content":" ID\tContext\tDescription\tExpected ResultT900\tC01+(C02,C03,C04)\tCreate a new RDS database resource\tE25 T901\tC01+(C02,C03,C04)\tCreate a new S3 bucket resource\tE26 ","version":"Next","tagName":"h3"},{"title":"Videos","type":0,"sectionRef":"#","url":"/docs/videos","content":"","keywords":"","version":"Next"},{"title":"YouTube playlist​","type":1,"pageTitle":"Videos","url":"/docs/videos#youtube-playlist","content":"   ","version":"Next","tagName":"h2"},{"title":"Decks​","type":1,"pageTitle":"Videos","url":"/docs/videos#decks","content":" OPA on AWS - Part 1 - IntroductionOPA on AWS - Part 2 - Platform engineeringOPA on AWS - Part 3 - Application developerOPA on AWS - Part 4 - Deploy Application to other environmentsOPA on AWS - Part 5 - Provision AWS Resources and resource bindingOPA on AWS - Part 6 - Deploying Serverless APIsOPA on AWS - Part 7 - OPA Security and RBACOPA on AWS - Part 8 - Orchestrating and Managing CI-CDOPA on AWS - Part 9 - Using Terraform to Orchestrate ApplicationsOPA on AWS - Part 10 - Customizing your platform - Adding providersOPA on AWS - Part 11 - Customizing your platform - Adding Templates  ","version":"Next","tagName":"h3"},{"title":"Individual videos​","type":1,"pageTitle":"Videos","url":"/docs/videos#individual-videos","content":" ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Videos","url":"/docs/videos#introduction","content":"   ","version":"Next","tagName":"h3"},{"title":"Platform Engineering​","type":1,"pageTitle":"Videos","url":"/docs/videos#platform-engineering","content":"   ","version":"Next","tagName":"h3"},{"title":"Application Developer​","type":1,"pageTitle":"Videos","url":"/docs/videos#application-developer","content":"   ","version":"Next","tagName":"h3"},{"title":"Deploy an Application to other Environments​","type":1,"pageTitle":"Videos","url":"/docs/videos#deploy-an-application-to-other-environments","content":"   ","version":"Next","tagName":"h3"},{"title":"Provision AWS Resources and Resource Binding​","type":1,"pageTitle":"Videos","url":"/docs/videos#provision-aws-resources-and-resource-binding","content":"   ","version":"Next","tagName":"h3"},{"title":"Deploying Serverless APIs​","type":1,"pageTitle":"Videos","url":"/docs/videos#deploying-serverless-apis","content":"   ","version":"Next","tagName":"h3"},{"title":"OPA Security and RBAC​","type":1,"pageTitle":"Videos","url":"/docs/videos#opa-security-and-rbac","content":"   ","version":"Next","tagName":"h3"},{"title":"Orchestrating and Managing CI/CD​","type":1,"pageTitle":"Videos","url":"/docs/videos#orchestrating-and-managing-cicd","content":"   ","version":"Next","tagName":"h3"},{"title":"Using Terraform to Orchestrate Applications​","type":1,"pageTitle":"Videos","url":"/docs/videos#using-terraform-to-orchestrate-applications","content":"   ","version":"Next","tagName":"h3"},{"title":"Customizing your platform - Adding providers​","type":1,"pageTitle":"Videos","url":"/docs/videos#customizing-your-platform---adding-providers","content":"   ","version":"Next","tagName":"h3"},{"title":"Customizing your platform - Adding Templates​","type":1,"pageTitle":"Videos","url":"/docs/videos#customizing-your-platform---adding-templates","content":"  ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}